{"ticket_id":"5680042","comments":[{"author_id":20602910429969,"body":"Hi Ken Lam,\n\u0026nbsp;\nThank you for reaching out to us. To protect your data from social engineering attempts, and to ensure that only designated individuals have access to your account, all Datto partners, their employees, and their authorized users must authenticate via an email process initiated by Datto Technical Support. Please provide the six character PIN code that was emailed to you.\n\u0026nbsp;\nOnce authorized, we can proceed with assisting you with your issue. You may call our support line or submit a chat with us if you require immediate assistance.\n\u0026nbsp;\nThank you,\n\u0026nbsp;\n\u0026nbsp;\n\n\n  Alexandre Desrosiers | Technical Support\n\nDatto, a Kaseya Company\n\nNA: +1 (888) 294-6312\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-07-22T17:50:38Z"},{"author_id":20602910429969,"body":"SUCCESS! PIN: 7PZ2D3\n\u0026nbsp;\nverbal auth'd: klam@rationalsolutions.com","created_at":"2024-07-22T17:51:29Z"},{"author_id":20602910429969,"body":"printf \"%-26s%-18s\\n\" \"Boot Date\" \"IBU Version\"; for i in $(ls /home/configBackup/.zfs/snapshot/*/files/var/log/kern* /var/log/kern*); do IFS=$''; result=($(zegrep -ah 'Kernel command line: BOOT_IMAGE=\\(loop\\)\\/vmlinuz' $i)); unset IFS; if [[ -n $result ]]; then for x in \"${result[@]}\"; do bootDate=$(echo $x  | grep -oE \"^[a-zA-Z]{3} [0-9]{1,2} [0-9]{2}:[0-9]{2}:[0-9]{2}\"); bootIbu=$(echo $x | grep -oP \"(?\u0026lt;=(loop=\\/images\\/))[0-9]{3,4}\\.[0-9]\"); printf \"%-26s%-18s\\n\" \"$bootDate\" \"$bootIbu\"; done; fi; done\nBoot Date                 IBU Version\nJan 24 02:18:25           2510.0\nFeb 7 01:34:27            2510.0\nFeb 21 00:50:38           2510.0\nFeb 23 22:22:48           2536.0\nMar 6 00:06:41            2536.0\nMar 20 00:22:40           2536.0\nApr 2 23:38:39            2536.0\nApr 3 21:23:07            2586.0\nApr 16 22:54:52           2586.0\nApr 30 22:10:51           2586.0\nMay 5 10:24:44            2586.0\nMay 19 09:40:54           2586.0\nMay 21 03:05:04           2630.0\nMay 22 07:56:11           2630.0\nMay 29 14:20:29           2630.0\nMay 30 10:42:51           2630.0\nJun 13 09:59:01           2630.0\nJun 17 10:56:14           2630.0\nJun 22 13:08:27           2630.0\nJul 4 08:21:35            2630.0\nJul 6 05:32:53            2668.0\n2668.0\nJul 10 10:17:38           2668.0\nJul 12 07:23:07           2668.0\nJul 14 13:12:27           2668.0\nJul 16 14:41:15           2668.0\nJul 22 12:02:22           2668.0 appliance updated to 2668 on 7/6 and has had multiple reboots since then.\u0026nbsp;\n\u0026nbsp;\n\n\u0026nbsp;\n ipmitool sel list\n   1 | 12/20/2022 | 15:35:10 | Event Logging Disabled #0x72 | Log area reset/cleared | Asserted\n   2 | 12/20/2022 | 15:46:51 | Drive Slot / Bay #0x99 | Drive Present () | Deasserted\n   3 | 12/20/2022 | 15:47:11 | Drive Slot / Bay #0x99 | Drive Present () | Asserted\n   4 | 12/20/2022 | 15:48:01 | Drive Slot / Bay #0x98 | Drive Present () | Deasserted\n   5 | 12/20/2022 | 15:48:01 | Drive Slot / Bay #0x9a | Drive Present () | Deasserted\n   6 | 12/20/2022 | 15:56:06 | Drive Slot / Bay #0x99 | Drive Present () | Deasserted\n   7 | 12/21/2022 | 10:11:41 | Drive Slot / Bay #0x98 | Drive Present () | Asserted\n   8 | 12/21/2022 | 10:11:46 | Drive Slot / Bay #0x99 | Drive Present () | Asserted\n   9 | 12/21/2022 | 10:11:46 | Drive Slot / Bay #0x9a | Drive Present () | Asserted\n   a | 02/08/2023 | 17:01:33 | Power Supply #0x86 | Power Supply AC lost | Asserted\n   b | 02/08/2023 | 17:02:13 | Power Supply #0x77 | Redundancy Lost | Asserted\nalexandre.desrosiers@Weis-Backup02:/var/log/datto# ipmitool sel list |grep watch\nalexandre.desrosiers@Weis-Backup02:/var/log/datto# cat device.log |grep -i watchdog\nalexandre.desrosiers@Weis-Backup02:/var/log/datto# cat device.log |grep -i watch \u0026nbsp;\n\u0026nbsp;\n\n\u0026nbsp;\n zfs list\nNAME                                                                     USED  AVAIL     REFER  MOUNTPOINT\nhomePool                                                                6.19T   971G      136K  /homePool\nhomePool/.recv                                                            96K   971G       96K  /homePool/.recv\nhomePool/f898177883474688872d40412eb01a1f-1721671261-localverification    80K   971G     5.45T  /homePool/f898177883474688872d40412eb01a1f-1721671261-localverification\nhomePool/home                                                           6.19T   971G      104K  /home\nhomePool/home/agents                                                    6.19T   971G      112K  /home/agents\nhomePool/home/agents/b71c75516d1b4c06833aeb140bbf8317                    407G   971G      278G  /home/agents/b71c75516d1b4c06833aeb140bbf8317\nhomePool/home/agents/f898177883474688872d40412eb01a1f                   5.79T   971G     5.45T  /home/agents/f898177883474688872d40412eb01a1f\nhomePool/home/configBackup                                              1.19G   971G     72.9M  /home/configBackup\nhomePool/os                                                             4.59G   971G       96K  /homePool/os\nhomePool/os/base                                                        4.59G   971G     3.51G  -\nhomePool/transfer                                                         96K   971G       96K  /datto/transfer\nalexandre.desrosiers@Weis-Backup02:/var/log/datto# date -d@1721671261\nMon Jul 22 14:01:01 EDT 2024 \u0026nbsp;\n\u0026nbsp;\nAlexandre Desrosiers: https://kaseya.zendesk.com/agent/tickets/5680042\nposted in BCDR Support Swarms / 1. Main Swarm Channel on Monday, July 22, 2024 2:18 PM\n\u0026nbsp;\npartner confirmed there was a citywide power outaged last wednesday 7/17/24 @ 1:30pm. However, I do not see a reboot occurred on that date. Device powered back on after power restore and then rebooted again on 7/22/24","created_at":"2024-07-22T18:23:22Z"},{"author_id":20602910429969,"body":"Per the assessment of Alexandre Desrosiers, this ticket requires assignment the Dedicated Level 2 team. This ticket will be tagged as such.\n\u0026nbsp;\nPlease fill out the following information below or your escalation may be rejected:\n\nName of Requester: Alexandre Desrosiers\nI have followed the L1 workflow: yes\u0026nbsp;\n\u0026nbsp;\nProblem Statement: Device continues to reboot. cause unknown\n\u0026nbsp;\nAffected Agent: Weis-Backup02 E43D1AA7924C\n\u0026nbsp;\nHow this issue began: device upgraded to 2668 IBU on 7/6 and has rebooted multiple times afterwards. device holds encrypted agents.\u0026nbsp;\n\u0026nbsp;\nWhat I've done so far: Checked for device upgrades, checked device logs, confirmed watchdog is not enabled so reboot are not due to that feature. searched for PTs, swarmed.","created_at":"2024-07-22T18:39:23Z"},{"author_id":20602910429969,"body":"Hello Ken,\u0026nbsp;\n\u0026nbsp;\nThank you for reaching out to Datto support for your request. As per our phone conversation, you would like us to investigate the cause for your Datto's spontaneuos reboots. We confirmed that you have encryted agents and the device is not undergoing updates during the time of the reboots. During our call I was not able to find any thing definitive that could explain the cause for this behavior. We agreed, that you would allow us to continue to troubleshoot this internally and follow up with you once we are able to determine what could be impacting this.\u0026nbsp;\n\u0026nbsp;\nThank you for remaining patient while we look into this issue for you. If this is an urgent issue, please call our 24/7 support line for immediate assistance, at +1 (833) 863 2237.\n\u0026nbsp;\nThank you,\n\n\n  Alexandre Desrosiers | Technical Support\n\nDatto, a Kaseya Company\n\nNA: +1 (888) 294-6312\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-07-22T18:43:53Z"},{"author_id":11408459599889,"body":"This case has been identified to meet one of the following criteria for de-escalation:\u0026nbsp;\n(NOTE: If the case is partner escalated, it stays with L2)\n\u0026nbsp;\n  Device Migration, Approved RMA, Sales Ops / BIS related inquiry, Reverse Roundtrip sync, Reverse roundtrip archive restore request(Non-DR).  \u0026nbsp;\nPlease state why this case is being sent back to L1:\u0026nbsp;\n  Swarm is in process with the tech and L2  \u0026nbsp;\nNext Steps:\n  Please look through device logs on the system, look for any issues regarding the pool/hardware or any call traces.\u0026nbsp;  \u0026nbsp;\nIf you are stuck, please reach out to your supervisor directly. Thank you.\u0026nbsp;","created_at":"2024-07-22T18:53:03Z"},{"author_id":20602910429969,"body":"kernel logs last reboot was 7/22/24 @ 12:02pm:\n\u0026nbsp;\n cation-queue-check comm=\"systemd\" exe=\"/usr/lib/systemd/systemd\" hostname=? addr=? terminal=? res=success'\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069550] kauditd_printk_skb: 430 callbacks suppressed\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069555] audit: type=1101 audit(1721620811.016:596836): pid=1530529 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069560] audit: audit_lost=682956 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069564] audit: kauditd hold queue overflow\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069638] audit: type=1123 audit(1721620811.016:596837): pid=1530529 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069642] audit: audit_lost=682957 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069644] audit: kauditd hold queue overflow\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069830] audit: type=1110 audit(1721620811.016:596838): pid=1530529 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069834] audit: audit_lost=682958 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069836] audit: kauditd hold queue overflow\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069961] audit: type=1105 audit(1721620811.016:596839): pid=1530529 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.155830] kauditd_printk_skb: 8 callbacks suppressed\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.155835] audit: type=1101 audit(1721620826.100:596842): pid=1530560 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.155840] audit: audit_lost=682962 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.155844] audit: kauditd hold queue overflow\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.156052] audit: type=1123 audit(1721620826.104:596843): pid=1530560 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.156056] audit: audit_lost=682963 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.156058] audit: kauditd hold queue overflow\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.156532] audit: type=1110 audit(1721620826.104:596844): pid=1530560 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.156536] audit: audit_lost=682964 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.156538] audit: kauditd hold queue overflow\nJul 22 00:00:26 Weis-Backup02 kernel: [465564.156824] audit: type=1105 audit(1721620826.104:596845): pid=1530560 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241505] kauditd_printk_skb: 14 callbacks suppressed\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241509] audit: type=1101 audit(1721620841.188:596850): pid=1530599 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241513] audit: audit_lost=682970 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241516] audit: kauditd hold queue overflow\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241612] audit: type=1123 audit(1721620841.188:596851): pid=1530599 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241614] audit: audit_lost=682971 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241616] audit: kauditd hold queue overflow\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241845] audit: type=1110 audit(1721620841.188:596852): pid=1530599 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241848] audit: audit_lost=682972 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241849] audit: kauditd hold queue overflow\nJul 22 00:00:41 Weis-Backup02 kernel: [465579.241990] audit: type=1105 audit(1721620841.188:596853): pid=1530599 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.326584] kauditd_printk_skb: 8 callbacks suppressed\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.326589] audit: type=1101 audit(1721620856.272:596856): pid=1530629 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.326594] audit: audit_lost=682976 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.326597] audit: kauditd hold queue overflow\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.326728] audit: type=1123 audit(1721620856.272:596857): pid=1530629 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.326731] audit: audit_lost=682977 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.326733] audit: kauditd hold queue overflow\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.327061] audit: type=1110 audit(1721620856.272:596858): pid=1530629 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.327064] audit: audit_lost=682978 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.327066] audit: kauditd hold queue overflow\nJul 22 00:00:56 Weis-Backup02 kernel: [465594.327275] audit: type=1105 audit(1721620856.272:596859): pid=1530629 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.988888] kauditd_printk_skb: 8 callbacks suppressed\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.988892] audit: type=1101 audit(1721620861.936:596862): pid=1530638 uid=0 auid=4294967295 ses=4294967295 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"root\" exe=\"/usr/sbin/cron\" hostname=? addr=? terminal=cron res=success'\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.988897] audit: audit_lost=682982 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.988900] audit: kauditd hold queue overflow\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.989059] audit: type=1101 audit(1721620861.936:596863): pid=1530639 uid=0 auid=4294967295 ses=4294967295 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"root\" exe=\"/usr/sbin/cron\" hostname=? addr=? terminal=cron res=success'\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.989062] audit: audit_lost=682983 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.989065] audit: kauditd hold queue overflow\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.989528] audit: type=1103 audit(1721620861.936:596864): pid=1530638 uid=0 auid=4294967295 ses=4294967295 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/sbin/cron\" hostname=? addr=? terminal=cron res=success'\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.989531] audit: audit_lost=682984 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.989533] audit: kauditd hold queue overflow\nJul 22 00:01:01 Weis-Backup02 kernel: [465599.989603] audit: type=1006 audit(1721620861.936:596865): pid=1530638 uid=0 subj=unconfined old-auid=4294967295 auid=0 tty=(none) old-ses=4294967295 ses=19806 res=1\nJul 22 00:01:11 Weis-Backup02 kernel: [465609.413756] kauditd_printk_skb: 66 callbacks suppressed\nJul 22 00:01:11 Weis-Backup02 kernel: [465609.413761] audit: type=1101 audit(1721620871.361:596882): pid=1530982 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:01:11 Weis-Backup02 kernel: [465609.413765] audit: audit_lost=683008 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:01:11 Weis-Backup02 kernel: [465609.413767] audit: kauditd hold queue overflow\nJul 22 00:01:11 Weis-Backup02 kernel: [465609.413872] audit: type=1123 audit(1721620871.361:596883): pid=1530982 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 22 00:01:11 Weis-Backup02 kernel: [465609.413874] audit: audit_lost=683009 audit_rate_limit=0 audit_backlog_limit=64 \u0026nbsp;\nsyslog:\n\u0026nbsp;\n Jul 22 00:00:02 Weis-Backup02 snapctl[1529532]: Checking if migration is scheduled\nJul 22 00:00:02 Weis-Backup02 snapctl[1529532]: #033[32m24-07-22 00:00:02#033[0m #033[36mMIG0001#033[0m #033[1;31mdevice-general#033[0m fa798b5b #033[1;34m...ration/MigrationService(2)#033[0m:#033[1;35m87#033[0m #033[1;30mDEBU#033[0m Checking to see if migration is scheduled...\nJul 22 00:00:02 Weis-Backup02 snapctl[1529538]: #033[32m24-07-22 00:00:02#033[0m #033[36mROB0000#033[0m #033[1;31mdevice-general#033[0m aeb79f6b #033[1;34m...OldBackupReportsCommand(2)#033[0m:#033[1;35m35#033[0m #033[1;36mINFO#033[0m Removing old backup reports\nJul 22 00:00:02 Weis-Backup02 snapctl[1529532]: #033[32m24-07-22 00:00:02#033[0m #033[36mMIG0016#033[0m #033[1;31mdevice-general#033[0m fa798b5b #033[1;34m...ation/MigrationService(2)#033[0m:#033[1;35m517#033[0m #033[1;30mDEBU#033[0m No migration in progress, lock acquired\nJul 22 00:00:02 Weis-Backup02 snapctl[1529532]: #033[32m24-07-22 00:00:02#033[0m #033[36mMIG0006#033[0m #033[1;31mdevice-general#033[0m fa798b5b #033[1;34m...ation/MigrationService(2)#033[0m:#033[1;35m124#033[0m #033[1;30mDEBU#033[0m Migration not scheduled to run\nJul 22 00:00:02 Weis-Backup02 snapctl[1529535]: Did not find Dattodrive/Owncloud data on the device\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-snapnas-growth-report.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: man-db.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: Finished Daily man-db regeneration.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-owncloud-removal.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: Finished Removes DattoDrive/OwnCloud after migrating user data to NasShare.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-migrate-schedule-run.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: logrotate.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: Finished Rotate log files.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-asset-backup-schedule-queue.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 snapctl[1529525]: #033[32m24-07-22 00:00:02#033[0m #033[36mBSR0012#033[0m #033[1;31m       WEISSRV#033[0m c3208da0 #033[1;34m...ckupScheduleRunService(2)#033[0m:#033[1;35m164#033[0m #033[1;30mDEBU#033[0m Backup Schedule Stats\nJul 22 00:00:02 Weis-Backup02 snapctl[1529525]: #033[1;30m                                                                                   CTXT#033[0m currentBackupCount: 0\nJul 22 00:00:02 Weis-Backup02 snapctl[1529525]:                                                                                         maxConcurrentBackups: 2\nJul 22 00:00:02 Weis-Backup02 snapctl[1529525]:                                                                                         possibleNewBackups: 2\nJul 22 00:00:02 Weis-Backup02 snapctl[1529525]:                                                                                         scheduleLength: 0\nJul 22 00:00:02 Weis-Backup02 snapctl[1529531]: Sleeping 2328 seconds for task fuzzing\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-asset-backup-schedule-run.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 snapctl[1529528]: #033[32m24-07-22 00:00:02#033[0m #033[36mVER0121#033[0m #033[1;31mdevice-general#033[0m 62afadad #033[1;34m...cationQueueCheckCommand(2)#033[0m:#033[1;35m81#033[0m #033[1;30mDEBU#033[0m Running the verification in screen \"screenshotHandler\"\nJul 22 00:00:02 Weis-Backup02 snapctl[1529528]: #033[32m24-07-22 00:00:02#033[0m #033[36mSCN0002#033[0m #033[1;31mdevice-general#033[0m 62afadad #033[1;34m...vice/src/Utility/Screen(2)#033[0m:#033[1;35m94#033[0m #033[1;30mDEBU#033[0m Starting screen.\nJul 22 00:00:02 Weis-Backup02 snapctl[1529528]: #033[1;30m                                                                                   CTXT#033[0m commandLine: 'screen' '-dmS' 'screenshotHandler' 'snapctl' 'asset:verification:queue:check'\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-asset-verification-queue-check.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: Finished Run verification on the next asset in the verification queue, if there is one..\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-reconcile-asset-removals.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: Finished Process any assets that have been marked for removal..\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-remove-old-backup-reports.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 snapctl[1529529]: #033[32m24-07-22 00:00:02#033[0m #033[36mTCC0000#033[0m #033[1;31mdevice-general#033[0m e73f8208 #033[1;34m...ndwidth/TrafficControl(2)#033[0m:#033[1;35m137#033[0m #033[1;36mINFO#033[0m Applied bandwidth limit\nJul 22 00:00:02 Weis-Backup02 snapctl[1529529]: #033[1;30m                                                                                   CTXT#033[0m remoteIP: 206.201.134.89\nJul 22 00:00:02 Weis-Backup02 snapctl[1529529]:                                                                                         port: eth0\nJul 22 00:00:02 Weis-Backup02 snapctl[1529529]:                                                                                         limitKbps: 20000\nJul 22 00:00:02 Weis-Backup02 snapctl[1529529]: Offsite bandwidth restrictions applied.\nJul 22 00:00:02 Weis-Backup02 snapctl[1529540]: #033[32m24-07-22 00:00:02#033[0m #033[36mSMC0002#033[0m #033[1;31mdevice-general#033[0m b6ba10b9 #033[1;34m.../SpeedSync/CheckCommand(2)#033[0m:#033[1;35m51#033[0m #033[1;30mDEBU#033[0m Device-wide SpeedSync pause is not in place.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-bandwidth-schedule.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 snapctl[1529542]: #033[32m24-07-22 00:00:02#033[0m #033[36mZCU0002#033[0m #033[1;31mdevice-general#033[0m 63523b58 #033[1;34m...Zfs/Cache/UpdateCommand(2)#033[0m:#033[1;35m53#033[0m #033[1;30mDEBU#033[0m Cache update complete\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-zfs-cache-update.service: Succeeded.\nJul 22 00:00:02 Weis-Backup02 systemd[1]: datto-system-speedsync-check.service: Succeeded.\nJul 22 00:00:03 Weis-Backup02 snapctl[1529527]: #033[32m24-07-22 00:00:03#033[0m #033[36mVRS3007#033[0m #033[1;31m  WEISSRV-HOST#033[0m 05dcca4f #033[1;34m.../VerificationScheduler(2)#033[0m:#033[1;35m132#033[0m #033[1;36mINFO#033[0m Checking if any snapshots need to be verified for asset ...\nJul 22 00:00:03 Weis-Backup02 snapctl[1529527]: #033[32m24-07-22 00:00:03#033[0m #033[36mVRS3020#033[0m #033[1;31m  WEISSRV-HOST#033[0m 05dcca4f #033[1;34m.../VerificationScheduler(2)#033[0m:#033[1;35m225#033[0m #033[1;30mDEBU#033[0m Offsite Point does not need screenshot.\nJul 22 00:00:03 Weis-Backup02 snapctl[1529527]: #033[1;30m                                                                                   CTXT#033[0m snapshotTime: 1721577602\nJul 22 00:00:04 Weis-Backup02 snapctl[1529527]: #033[32m24-07-22 00:00:04#033[0m #033[36mVRS3007#033[0m #033[1;31m       WEISSRV#033[0m 05dcca4f #033[1;34m.../VerificationScheduler(2)#033[0m:#033[1;35m132#033[0m #033[1;36mINFO#033[0m Checking if any snapshots need to be verified for asset ...\nJul 22 00:00:04 Weis-Backup02 snapctl[1529527]: #033[32m24-07-22 00:00:04#033[0m #033[36mVRS3020#033[0m #033[1;31m       WEISSRV#033[0m 05dcca4f #033[1;34m.../VerificationScheduler(2)#033[0m:#033[1;35m225#033[0m #033[1;30mDEBU#033[0m Offsite Point does not need screenshot.\nJul 22 00:00:04 Weis-Backup02 snapctl[1529527]: #033[1;30m                                                                                   CTXT#033[0m snapshotTime: 1721577602\nJul 22 00:00:04 Weis-Backup02 systemd[1]: datto-asset-verification-check.service: Succeeded.\nJul 22 00:00:09 Weis-Backup02 td-agent-bit[9437]: [2024/07/22 00:00:09] [ info] [input:tail:tail.0] inotify_fs_remove(): inode=135760 watch_fd=260\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069550] kauditd_printk_skb: 430 callbacks suppressed\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069555] audit: type=1101 audit(1721620811.016:596836): pid=1530529 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069560] audit: audit_lost=682956 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069564] audit: kauditd hold queue overflow\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069638] audit: type=1123 audit(1721620811.016:596837): pid=1530529 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069642] audit: audit_lost=682957 audit_rate_limit=0 audit_backlog_limit=64\nJul 22 00:00:11 Weis-Backup02 kernel: [465549.069644] audit: kauditd hold queue overflow \u0026nbsp;","created_at":"2024-07-22T20:10:43Z"},{"author_id":22294140476049,"body":"Crash on July 6th, 10th, 12th, 14th, 16th,\u0026nbsp;\n alexander.cottrell@Weis-Backup02:~# last\nroot     pts/4        127.0.0.1        Tue Jul 23 12:31    gone - no logout\nroot     pts/2        127.0.0.1        Mon Jul 22 15:59 - 17:10  (01:10)\nroot     pts/2        127.0.0.1        Mon Jul 22 13:52 - 14:51  (00:58)\nstatusus tty1                          Mon Jul 22 12:02   still logged in\nreboot   system boot  5.15.0-105-gener Mon Jul 22 12:02   still running\nstatusus tty1                          Tue Jul 16 14:41 - crash (5+21:20)\nreboot   system boot  5.15.0-105-gener Tue Jul 16 14:41   still running\nstatusus tty1                          Sun Jul 14 13:12 - crash (2+01:28)\nreboot   system boot  5.15.0-105-gener Sun Jul 14 13:12   still running\nstatusus tty1                          Fri Jul 12 07:23 - crash (2+05:49)\nreboot   system boot  5.15.0-105-gener Fri Jul 12 07:22   still running\nstatusus tty1                          Wed Jul 10 10:17 - crash (1+21:05)\nreboot   system boot  5.15.0-105-gener Wed Jul 10 10:17   still running\nstatusus tty1                          Sat Jul  6 08:27 - crash (4+01:50)\nreboot   system boot  5.15.0-105-gener Sat Jul  6 08:27   still running\nstatusus tty1                          Sat Jul  6 05:32 - crash  (02:54)\nreboot   system boot  5.15.0-105-gener Sat Jul  6 05:32   still running\n\nwtmp begins Sat Jul  6 05:32:30 2024 \u0026nbsp;\nNothing in ipmitool for anything recent\n alexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1718611835/files/var/log# ipmitool sel list\n   1 | 12/20/2022 | 15:35:10 | Event Logging Disabled #0x72 | Log area reset/cleared | Asserted\n   2 | 12/20/2022 | 15:46:51 | Drive Slot / Bay #0x99 | Drive Present () | Deasserted\n   3 | 12/20/2022 | 15:47:11 | Drive Slot / Bay #0x99 | Drive Present () | Asserted\n   4 | 12/20/2022 | 15:48:01 | Drive Slot / Bay #0x98 | Drive Present () | Deasserted\n   5 | 12/20/2022 | 15:48:01 | Drive Slot / Bay #0x9a | Drive Present () | Deasserted\n   6 | 12/20/2022 | 15:56:06 | Drive Slot / Bay #0x99 | Drive Present () | Deasserted\n   7 | 12/21/2022 | 10:11:41 | Drive Slot / Bay #0x98 | Drive Present () | Asserted\n   8 | 12/21/2022 | 10:11:46 | Drive Slot / Bay #0x99 | Drive Present () | Asserted\n   9 | 12/21/2022 | 10:11:46 | Drive Slot / Bay #0x9a | Drive Present () | Asserted\n   a | 02/08/2023 | 17:01:33 | Power Supply #0x86 | Power Supply AC lost | Asserted\n   b | 02/08/2023 | 17:02:13 | Power Supply #0x77 | Redundancy Lost | Asserted \u0026nbsp;\nPSU is fine\n alexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1718611835/files/var/log# ipmitool sdr type \"Power Supply\"\nPS Redundancy    | 77h | ok  |  7.1 | Fully Redundant\nStatus           | 85h | ok  | 10.1 | Presence detected\nStatus           | 86h | ok  | 10.2 | Presence detected \u0026nbsp;\nNo CMOS sensor?\n alexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1718611835/files/var/log# ipmitool sensor get P_VBAT\nLocating sensor record...\nSensor data record \"P_VBAT\" not found! \u0026nbsp;\nChecking device.logs for crash point on July 12th\n alexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1720858054/files/var/log/datto# zcat device.log.*.gz | grep -i 07:23\n{\"message\":\"Transferring data, 5647523840/10031792128 [ 56 ]   Timeout Countdown: 26-900\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"230948c5\",\"level\":\"DEBUG\",\"logCode\":\"BAK2000\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T12:07:23.814+00:00\"}\n{\"message\":\"Running the verification in screen \\\"screenshotHandler\\\"\",\"channel\":\"device-general\",\"contextId\":\"58a687dc\",\"level\":\"DEBUG\",\"logCode\":\"VER0121\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:01.857+00:00\"}\n{\"message\":\"Starting screen.\",\"context\":{\"commandLine\":\"'screen' '-dmS' 'screenshotHandler' 'snapctl' 'asset:verification:queue:check'\"},\"channel\":\"device-general\",\"contextId\":\"58a687dc\",\"level\":\"DEBUG\",\"logCode\":\"SCN0002\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:01.858+00:00\"}\n{\"message\":\"Backup Schedule Stats\",\"context\":{\"currentBackupCount\":0,\"maxConcurrentBackups\":2,\"possibleNewBackups\":2,\"scheduleLength\":0},\"channel\":\"WEISSRV\",\"contextId\":\"1ec2b3ee\",\"level\":\"DEBUG\",\"logCode\":\"BSR0012\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:01.859+00:00\"}\n{\"message\":\"Device-wide SpeedSync pause is not in place.\",\"channel\":\"device-general\",\"contextId\":\"683ee77e\",\"level\":\"DEBUG\",\"logCode\":\"SMC0002\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:01.902+00:00\"}\n{\"message\":\"No reboot scheduled.\",\"channel\":\"device-general\",\"contextId\":\"e70acc2e\",\"level\":\"DEBUG\",\"logCode\":\"PWM0001\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:02.032+00:00\"}\n{\"message\":\"Starting the verification process...\",\"channel\":\"device-general\",\"contextId\":\"2f07a4a6\",\"level\":\"DEBUG\",\"logCode\":\"SCN0970\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:02.057+00:00\"}\n{\"message\":\"No verifications queued.\",\"channel\":\"device-general\",\"contextId\":\"2f07a4a6\",\"level\":\"DEBUG\",\"logCode\":\"SCN4005\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:02.057+00:00\"}\n{\"message\":\"Call to emergency checkin was successful\",\"channel\":\"device-general\",\"contextId\":\"a0f4bbc7\",\"level\":\"INFO\",\"logCode\":\"ECN0001\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:40.867+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720699200 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.722+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720702801 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.722+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720706401 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.722+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720710001 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.722+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720713601 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720717202 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720720802 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720724461 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720728001 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720731662 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720735202 because: isAlreadyOffsite\",\"channel\":\"WEISSRV-HOST\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720699200 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720702801 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720706401 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.723+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720710001 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.724+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720713601 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.724+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720717202 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.724+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720720802 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.724+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720724461 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.724+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720728001 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.724+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720731662 because: isEarlierThanConnectingPoint\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-1207:23:47.724+00:00\"}\n{\"message\":\"Skipping offsite for point: 1720735202 because: isAlreadyOffsite\",\"channel\":\"WEISSRV\",\"contextId\":\"2038c693\",\"level\":\"DEBUG\",\"logCode\":\"SYN1403\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T07:23:47.724+00:00\"}\n{\"message\":\"Sending request to the inspection service.\",\"context\":{\"method\":\"LoginManager:status\",\"contents\":null},\"channel\":\"WEISSRV\",\"contextId\":\"6a196286\",\"level\":\"DEBUG\",\"logCode\":\"XXX0100\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-12T23:07:23.009+00:00\"}\nalexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1720858054/files/var/log/datto# \u0026nbsp;\nChecking kern logs\n 320686F6D65506F6F6C terminal=tty1 res=success'\nJul 12 07:14:57 Weis-Backup02 kernel: [161852.696413] audit: audit_lost=241859 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:14:57 Weis-Backup02 kernel: [161852.696414] audit: kauditd hold queue overflow\nJul 12 07:14:57 Weis-Backup02 kernel: [161852.696753] audit: type=1110 audit(1720782897.262:211396): pid=2179523 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:14:57 Weis-Backup02 kernel: [161852.696754] audit: audit_lost=241860 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:14:57 Weis-Backup02 kernel: [161852.696755] audit: kauditd hold queue overflow\nJul 12 07:14:57 Weis-Backup02 kernel: [161852.696970] audit: type=1105 audit(1720782897.262:211397): pid=2179523 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.783368] kauditd_printk_skb: 177 callbacks suppressed\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.783374] audit: type=1101 audit(1720782912.350:211443): pid=2179911 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.783380] audit: audit_lost=241922 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.783384] audit: kauditd hold queue overflow\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.783570] audit: type=1123 audit(1720782912.350:211444): pid=2179911 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.783574] audit: audit_lost=241923 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.783577] audit: kauditd hold queue overflow\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.784040] audit: type=1110 audit(1720782912.350:211445): pid=2179911 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.784043] audit: audit_lost=241924 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.784045] audit: kauditd hold queue overflow\nJul 12 07:15:12 Weis-Backup02 kernel: [161867.784342] audit: type=1105 audit(1720782912.350:211446): pid=2179911 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871081] kauditd_printk_skb: 8 callbacks suppressed\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871084] audit: type=1101 audit(1720782927.438:211449): pid=2179941 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871088] audit: audit_lost=241928 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871090] audit: kauditd hold queue overflow\nBinary file (standard input) matches \u0026nbsp;\nsyslog\n Jul 12 07:15:27 Weis-Backup02 kernel: [161882.871197] audit: audit_lost=241929 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871199] audit: kauditd hold queue overflow\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871435] audit: type=1110 audit(1720782927.438:211451): pid=2179941 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871439] audit: audit_lost=241930 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871440] audit: kauditd hold queue overflow\nJul 12 07:15:27 Weis-Backup02 kernel: [161882.871589] audit: type=1105 audit(1720782927.438:211452): pid=2179941 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:29 Weis-Backup02 systemd[1]: Starting Checks the TLS certificate(s) for the local Mercury FTP server...\nJul 12 07:15:29 Weis-Backup02 systemd[1]: datto-mercuryftp-tls-check.service: Succeeded.\nJul 12 07:15:29 Weis-Backup02 systemd[1]: Finished Checks the TLS certificate(s) for the local Mercury FTP server.\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.951739] kauditd_printk_skb: 14 callbacks suppressed\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.951744] audit: type=1101 audit(1720782942.518:211457): pid=2180006 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.951750] audit: audit_lost=241936 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.951754] audit: kauditd hold queue overflow\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.951860] audit: type=1123 audit(1720782942.518:211458): pid=2180006 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.951863] audit: audit_lost=241937 audit_rate_limit=0 audit_backlog_limit=64\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.951866] audit: kauditd hold queue overflow\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.952117] audit: type=1110 audit(1720782942.518:211459): pid=2180006 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 12 07:15:42 Weis-Backup02 kernel: [161897.952121] audit: audit_lost=241938 audit_rate_limit=0 audit_backlog_limit=64\nBinary file (standard input) matches \u0026nbsp;\n\u0026nbsp;\nChecked for Crash point on 16th @ 14:41 in kern.log\n alexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1721203051/files/var/log# zcat kern.log.1.gz | grep -i \"14:41\"\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427113] audit: type=1101 audit(1721128481.250:194098): pid=1735942 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427118] audit: audit_lost=222537 audit_rate_limit=0 audit_backlog_limit=64\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427121] audit: kauditd hold queue overflow\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427197] audit: type=1123 audit(1721128481.250:194099): pid=1735942 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427201] audit: audit_lost=222538 audit_rate_limit=0 audit_backlog_limit=64\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427203] audit: kauditd hold queue overflow\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427370] audit: type=1110 audit(1721128481.250:194100): pid=1735942 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427373] audit: audit_lost=222539 audit_rate_limit=0 audit_backlog_limit=64\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427376] audit: kauditd hold queue overflow\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427487] audit: type=1105 audit(1721128481.250:194101): pid=1735942 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nBinary file (standard input) matches sys.log\n alexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1721203051/files/var/log# zcat syslog.1.gz | grep -i \"14:41\"\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427113] audit: type=1101 audit(1721128481.250:194098): pid=1735942 uid=1001 auid=1001 ses=1 subj=unconfined msg='op=PAM:accounting grantors=pam_permit acct=\"statususer\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427118] audit: audit_lost=222537 audit_rate_limit=0 audit_backlog_limit=64\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427121] audit: kauditd hold queue overflow\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427197] audit: type=1123 audit(1721128481.250:194099): pid=1735942 uid=1001 auid=1001 ses=1 subj=unconfined msg='cwd=\"/datto/statususer\" cmd=7A706F6F6C2073746174757320686F6D65506F6F6C terminal=tty1 res=success'\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427201] audit: audit_lost=222538 audit_rate_limit=0 audit_backlog_limit=64\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427203] audit: kauditd hold queue overflow\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427370] audit: type=1110 audit(1721128481.250:194100): pid=1735942 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:setcred grantors=pam_permit,pam_cap acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427373] audit: audit_lost=222539 audit_rate_limit=0 audit_backlog_limit=64\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427376] audit: kauditd hold queue overflow\nJul 16 07:14:41 Weis-Backup02 kernel: [151348.427487] audit: type=1105 audit(1721128481.250:194101): pid=1735942 uid=0 auid=1001 ses=1 subj=unconfined msg='op=PAM:session_open grantors=pam_env,pam_env,pam_permit,pam_umask,pam_unix,pam_winbind acct=\"root\" exe=\"/usr/bin/sudo\" hostname=Weis-Backup02 addr=? terminal=/dev/tty1 res=success'\nBinary file (standard input) matches device.logs\n alexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1721203051/files/var/log/datto# zcat device.log.*.gz | grep -i \"14:41\"\n{\"message\":\"Transferring data, 267157553152/5944339730432 [ 4 ]   Timeout Countdown: 0-900\",\"channel\":\"WEISSRV\",\"contextId\":\"0d0f2c5b\",\"level\":\"DEBUG\",\"logCode\":\"BAK2000\",\"user\":\"(CLI)\",\"@timestamp\":\"2024-07-16T21:14:41.962+00:00\"}\nBinary file (standard input) matches event.log shows errors, but nothing that indicates internal failure\n alexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1721203051/files/var/log/datto# zcat events.log.*.gz | grep -i 14:41\nalexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1721203051/files/var/log/datto# zcat events.log.*.gz | grep -i fail\nalexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1721203051/files/var/log/datto# zcat events.log.*.gz | grep -i error\n{\"source\":\"iris\",\"type\":\"device.log\",\"data.index\":777276,\"data.platform.model\":\"S5-8\",\"data.platform.imageVersion\":2668,\"data.platform.kernelVersion\":\"5.15.0-105-generic\",\"data.platform.irisVersion\":\"4.35.4\",\"data.platform.deviceRole\":\"partner\",\"data.platform.environment\":\"prod\",\"data.platform.datacenterRegion\":\"\",\"data.log.logLevel\":300,\"data.log.level\":\"WARNING\",\"data.log.logCode\":\"HTT9004\",\"context.logMessage\":\"Error checking HTTPS connectivity.\",\"context.userName\":\"(CLI)\",\"context.clientIp\":\"\",\"context.logContext.error\":\"cURL error 28: Resolving timed out after 2000 milliseconds (see https:\\/\\/curl.haxx.se\\/libcurl\\/c\\/libcurl-errors.html)\",\"context.logContext.errorCode\":\"0\",\"context.logContext.stackTrace\":\"#0 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/CurlFactory.php(155): GuzzleHttp\\\\Handler\\\\CurlFactory::createRejection()\\n#1 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/CurlFactory.php(105): GuzzleHttp\\\\Handler\\\\CurlFactory::finishError()\\n#2 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/CurlHandler.php(43): GuzzleHttp\\\\Handler\\\\CurlFactory::finish()\\n#3 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/Proxy.php(28): GuzzleHttp\\\\Handler\\\\CurlHandler-\u0026gt;__invoke()\\n#4 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/Proxy.php(51): GuzzleHttp\\\\Handler\\\\Proxy::GuzzleHttp\\\\Handler\\\\{closure}()\\n#5 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/PrepareBodyMiddleware.php(37): GuzzleHttp\\\\Handler\\\\Proxy::GuzzleHttp\\\\Handler\\\\{closure}()\\n#6 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Middleware.php(29): GuzzleHttp\\\\PrepareBodyMiddleware-\u0026gt;__invoke()\\n#7 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/RedirectMiddleware.php(70): GuzzleHttp\\\\Middleware::GuzzleHttp\\\\{closure}()\\n#8 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Middleware.php(59): GuzzleHttp\\\\RedirectMiddleware-\u0026gt;__invoke()\\n#9 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/HandlerStack.php(71): GuzzleHttp\\\\Middleware::GuzzleHttp\\\\{closure}()\\n#10 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Client.php(351): GuzzleHttp\\\\HandlerStack-\u0026gt;__invoke()\\n#11 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Client.php(162): GuzzleHttp\\\\Client-\u0026gt;transfer()\\n#12 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Client.php(182): GuzzleHttp\\\\Client-\u0026gt;requestAsync()\\n#13 \\/usr\\/lib\\/datto\\/device\\/src\\/Https\\/HttpsService.php(2) : eval()'d code(230): GuzzleHttp\\\\Client-\u0026gt;request()\\n#14 \\/usr\\/lib\\/datto\\/device\\/src\\/Https\\/HttpsService.php(2) : eval()'d code(343): Datto\\\\Https\\\\HttpsService-\u0026gt;checkConnectivity()\\n#15 \\/usr\\/lib\\/datto\\/device\\/src\\/Https\\/HttpsService.php(2) : eval()'d code(167): Datto\\\\Https\\\\HttpsService-\u0026gt;updateRedirectHost()\\n#16 \\/usr\\/lib\\/datto\\/device\\/src\\/App\\/Console\\/Command\\/Https\\/HttpsCheckCommand.php(2) : eval()'d code(46): Datto\",\"requestId\":\"2c016327\",\"resellerId\":13375,\"deviceId\":343238,\"host\":\"Weis-Backup02\",\"@timestamp\":\"2024-07-17T02:36:41-0400\",\"schemaVersion\":11}\n{\"source\":\"iris\",\"type\":\"device.log\",\"data.index\":777290,\"data.platform.model\":\"S5-8\",\"data.platform.imageVersion\":2668,\"data.platform.kernelVersion\":\"5.15.0-105-generic\",\"data.platform.irisVersion\":\"4.35.4\",\"data.platform.deviceRole\":\"partner\",\"data.platform.environment\":\"prod\",\"data.platform.datacenterRegion\":\"\",\"data.log.logLevel\":400,\"data.log.level\":\"ERROR\",\"data.log.logCode\":\"REP0005\",\"context.logMessage\":\"Error occurred during asset reconciliation\",\"context.userName\":\"(CLI)\",\"context.clientIp\":\"\",\"context.logContext.error\":\"Query for v1\\/device\\/asset\\/replication\\/getInbound returned with an error JSON-RPC $response['error'] = \\\"{\\\"code\\\":0,\\\"message\\\":\\\"An unexpected error occurred.\\\"}\\\"\",\"context.logContext.errorCode\":\"0\",\"context.logContext.stackTrace\":\"#0 \\/usr\\/lib\\/datto\\/device\\/src\\/Cloud\\/JsonRpcClient.php(2) : eval()'d code(158): Datto\\\\Cloud\\\\JsonRpcClient-\u0026gt;query()\\n#1 \\/usr\\/lib\\/datto\\/device\\/src\\/Replication\\/ReplicationService.php(2) : eval()'d code(211): Datto\\\\Cloud\\\\JsonRpcClient-\u0026gt;queryWithId()\\n#2 \\/usr\\/lib\\/datto\\/device\\/src\\/App\\/Console\\/Command\\/Asset\\/AssetReplicationReconcileCommand.php(2) : eval()'d code(48): Datto\\\\Replication\\\\ReplicationService-\u0026gt;reconcileAssets()\\n#3 \\/usr\\/lib\\/datto\\/device\\/vendor\\/symfony\\/console\\/Command\\/Command.php(298): Datto\\\\App\\\\Console\\\\Command\\\\Asset\\\\AssetReplicationReconcileCommand-\u0026gt;execute()\\n#4 \\/usr\\/lib\\/datto\\/device\\/vendor\\/symfony\\/console\\/Application.php(1058): Symfony\\\\Component\\\\Console\\\\Command\\\\Command-\u0026gt;run()\\n#5 \\/usr\\/lib\\/datto\\/device\\/vendor\\/symfony\\/framework-bundle\\/Console\\/Application.php(96): Symfony\\\\Component\\\\Console\\\\Application-\u0026gt;doRunCommand()\\n#6 \\/usr\\/lib\\/datto\\/device\\/vendor\\/symfony\\/console\\/Application.php(301): Symfony\\\\Bundle\\\\FrameworkBundle\\\\Console\\\\Application-\u0026gt;doRunCommand()\\n#7 \\/usr\\/lib\\/datto\\/device\\/vendor\\/symfony\\/framework-bundle\\/Console\\/Application.php(82): Symfony\\\\Component\\\\Console\\\\Application-\u0026gt;doRun()\\n#8 \\/usr\\/lib\\/datto\\/device\\/vendor\\/symfony\\/console\\/Application.php(171): Symfony\\\\Bundle\\\\FrameworkBundle\\\\Console\\\\Application-\u0026gt;doRun()\\n#9 \\/usr\\/lib\\/datto\\/device\\/app\\/snapctl.php(21): Symfony\\\\Component\\\\Console\\\\Application-\u0026gt;run()\\n#10 {main}\",\"requestId\":\"9f58e2fa\",\"resellerId\":13375,\"deviceId\":343238,\"host\":\"Weis-Backup02\",\"@timestamp\":\"2024-07-17T03:00:10-0400\",\"schemaVersion\":11}\n{\"source\":\"iris\",\"type\":\"device.log\",\"data.index\":777207,\"data.platform.model\":\"S5-8\",\"data.platform.imageVersion\":2668,\"data.platform.kernelVersion\":\"5.15.0-105-generic\",\"data.platform.irisVersion\":\"4.35.4\",\"data.platform.deviceRole\":\"partner\",\"data.platform.environment\":\"prod\",\"data.platform.datacenterRegion\":\"\",\"data.log.logLevel\":300,\"data.log.level\":\"WARNING\",\"data.log.logCode\":\"HTT9004\",\"context.logMessage\":\"Error checking HTTPS connectivity.\",\"context.userName\":\"(CLI)\",\"context.clientIp\":\"\",\"context.logContext.error\":\"cURL error 28: Resolving timed out after 2000 milliseconds (see https:\\/\\/curl.haxx.se\\/libcurl\\/c\\/libcurl-errors.html)\",\"context.logContext.errorCode\":\"0\",\"context.logContext.stackTrace\":\"#0 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/CurlFactory.php(155): GuzzleHttp\\\\Handler\\\\CurlFactory::createRejection()\\n#1 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/CurlFactory.php(105): GuzzleHttp\\\\Handler\\\\CurlFactory::finishError()\\n#2 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/CurlHandler.php(43): GuzzleHttp\\\\Handler\\\\CurlFactory::finish()\\n#3 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/Proxy.php(28): GuzzleHttp\\\\Handler\\\\CurlHandler-\u0026gt;__invoke()\\n#4 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Handler\\/Proxy.php(51): GuzzleHttp\\\\Handler\\\\Proxy::GuzzleHttp\\\\Handler\\\\{closure}()\\n#5 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/PrepareBodyMiddleware.php(37): GuzzleHttp\\\\Handler\\\\Proxy::GuzzleHttp\\\\Handler\\\\{closure}()\\n#6 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Middleware.php(29): GuzzleHttp\\\\PrepareBodyMiddleware-\u0026gt;__invoke()\\n#7 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/RedirectMiddleware.php(70): GuzzleHttp\\\\Middleware::GuzzleHttp\\\\{closure}()\\n#8 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Middleware.php(59): GuzzleHttp\\\\RedirectMiddleware-\u0026gt;__invoke()\\n#9 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/HandlerStack.php(71): GuzzleHttp\\\\Middleware::GuzzleHttp\\\\{closure}()\\n#10 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Client.php(351): GuzzleHttp\\\\HandlerStack-\u0026gt;__invoke()\\n#11 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Client.php(162): GuzzleHttp\\\\Client-\u0026gt;transfer()\\n#12 \\/usr\\/lib\\/datto\\/device\\/vendor\\/guzzlehttp\\/guzzle\\/src\\/Client.php(182): GuzzleHttp\\\\Client-\u0026gt;requestAsync()\\n#13 \\/usr\\/lib\\/datto\\/device\\/src\\/Https\\/HttpsService.php(2) : eval()'d code(230): GuzzleHttp\\\\Client-\u0026gt;request()\\n#14 \\/usr\\/lib\\/datto\\/device\\/src\\/Https\\/HttpsService.php(2) : eval()'d code(343): Datto\\\\Https\\\\HttpsService-\u0026gt;checkConnectivity()\\n#15 \\/usr\\/lib\\/datto\\/device\\/src\\/Https\\/HttpsService.php(2) : eval()'d code(167): Datto\\\\Https\\\\HttpsService-\u0026gt;updateRedirectHost()\\n#16 \\/usr\\/lib\\/datto\\/device\\/src\\/App\\/Console\\/Command\\/Https\\/HttpsCheckCommand.php(2) : eval()'d code(46): Datto\",\"requestId\":\"5427f23f\",\"resellerId\":13375,\"deviceId\":343238,\"host\":\"Weis-Backup02\",\"@timestamp\":\"2024-07-17T00:25:55-0400\",\"schemaVersion\":11}\nalexander.cottrell@Weis-Backup02:/home/configBackup/.zfs/snapshot/1721203051/files/var/log/datto#","created_at":"2024-07-23T17:31:36Z"},{"author_id":22294140476049,"body":"It started failing the day it upgraded\n Jul 4 08:21:35            2630.0\nJul 6 05:32:53            2668.0\n2668.0\nJul 10 10:17:38           2668.0\nJul 12 07:23:07           2668.0\nJul 14 13:12:27           2668.0\nJul 16 14:41:15           2668.0\nJul 22 12:02:22           2668.0 host/images\n alexander.cottrell@Weis-Backup02:~# ll /host/images\ntotal 14116940\ndrwxr-xr-x 2 root root        4096 Jul  6 05:34 ./\ndrwxr-xr-x 6 root root        4096 Feb  8  2023 ../\n-rw-r--r-- 1 root root 17179869184 Jul  6 05:32 2630.0.img\n-rw-r--r-- 1 root root           0 May 21 03:07 2630.0.img.success\n-rw-r--r-- 1 root root 17179869184 Jul 23 13:33 2668.0.img\n-rw-r--r-- 1 root root           0 Jul  6 05:34 2668.0.img.success \u0026nbsp;\nHurting my head. Swarmed\nAlexander Cottrell: Device keeps rebooting, can't find reason why\nposted in BCDR Support Swarms / 1. Main Swarm Channel on Tuesday, July 23, 2024 1:40 PM","created_at":"2024-07-23T17:41:39Z"},{"author_id":22294140476049,"body":"Hello Ken,\n\u0026nbsp;\nI took a look ALL over the timestamps of these crashes shown\n root     pts/4        127.0.0.1        Tue Jul 23 12:31    gone - no logout\u0026nbsp;\nroot     pts/2        127.0.0.1        Mon Jul 22 15:59 - 17:10  (01:10)\u0026nbsp;\nroot     pts/2        127.0.0.1        Mon Jul 22 13:52 - 14:51  (00:58)\u0026nbsp;\nstatusus tty1                          Mon Jul 22 12:02   still logged in\u0026nbsp;\nreboot   system boot  5.15.0-105-gener Mon Jul 22 12:02   still running\u0026nbsp;\nstatusus tty1                          Tue Jul 16 14:41 - crash (5+21:20)\u0026nbsp;\nreboot   system boot  5.15.0-105-gener Tue Jul 16 14:41   still running\u0026nbsp;\nstatusus tty1                          Sun Jul 14 13:12 - crash (2+01:28)\u0026nbsp;\nreboot   system boot  5.15.0-105-gener Sun Jul 14 13:12   still running\u0026nbsp;\nstatusus tty1                          Fri Jul 12 07:23 - crash (2+05:49)\u0026nbsp;\nreboot   system boot  5.15.0-105-gener Fri Jul 12 07:22   still running\u0026nbsp;\nstatusus tty1                          Wed Jul 10 10:17 - crash (1+21:05)\u0026nbsp;\nreboot   system boot  5.15.0-105-gener Wed Jul 10 10:17   still running\u0026nbsp;\nstatusus tty1                          Sat Jul  6 08:27 - crash (4+01:50)\u0026nbsp;\nreboot   system boot  5.15.0-105-gener Sat Jul  6 08:27   still running\u0026nbsp;\nstatusus tty1                          Sat Jul  6 05:32 - crash  (02:54)\u0026nbsp;\nreboot   system boot  5.15.0-105-gener Sat Jul  6 05:32   still running\u0026nbsp; \u0026nbsp;\nBut nothing in the sys.log or kernel logs or device logs didn't show me anything definitive that I can find so far.\nI do see that these crashes started the exact time the same time the firmware upgraded so I think thats close to the answer of why this is happening.\nI have my internal team looking into this and we'll get back to you with something more defnitive.\n\u0026nbsp;\nKind Regards,\n\n\n  Alexander Cottrell | | Technical Support \n Datto BCDR, a Kaseya Company \n NA: +1 (888) 294-6312\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637 \n helpdesk.kaseya.com","created_at":"2024-07-23T17:51:03Z"},{"author_id":16965125893265,"body":"Per the assessment of Alexis van der Biest, this ticket requires assignment the Dedicated Level 2 team. This ticket will be tagged as such.\n\u0026nbsp;\nPlease fill out the following information below or your escalation may be rejected:\n\nName of Requester: Alexis van der Biest\nI have followed the L1 workflow: (yes or no) Yes\n\u0026nbsp;\nProblem Statement:Device continues to reboot. cause unknown\n\u0026nbsp;\nAffected Agent: Weis-Backup02 E43D1AA7924C\n\u0026nbsp;\nHow this issue began: device upgraded to 2668 IBU on 7/6 and has rebooted multiple times afterwards. device holds encrypted agents.\u0026nbsp;\n\u0026nbsp;\nWhat I've done so far: This issue has been swaremd twice without a response. Techs have reviewedkern, device, dmesg and journalctl logs and are not able to find anything that points to why the device keeps rebooting.","created_at":"2024-07-23T20:27:11Z"},{"author_id":15089785937297,"body":"Hello Ken,\n\u0026nbsp;\nMy name is Maria and I am the elevated support technician who will be reviewing this case!\n\u0026nbsp;\nConsidering that the issue with the reboots started after the IBU upgrade to version 2668, it might be related to a corrupted IBU. Due to that, I would like to check with you if it would be possible for us to perform further troubleshooting steps while you are on-site with the device. During that process, we would boot from a live environment and fix the IBU image while it is unmounted.\n\u0026nbsp;\nIf you are able to go on-site so we can work through that, please give us a call while in there and have in hand an 8 GB+ USB stick imaged with the Datto Utilities, as well as a keyboard and monitor. I will attach a picture below of the ports available on your device (S5-8), as you might want to take that into consideration when picking the monitor/cable to be taken to the site.\n\u0026nbsp;\n\nPlease let us know if you have any further questions in the meantime.\n\u0026nbsp;\nThank you,\n\n\n  Maria Eduarda Joazeiro Gomes | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company \n\nUSA: (833) 832 4780 \n\nEMEA: +44 (0) 118 402 9609 \n\nAustralia: +61 (0)2 801 56826 \n\nSingapore: 800-852-3047 \n\nhelpdesk.kaseya.com","created_at":"2024-07-23T20:53:03Z"},{"author_id":11408459602705,"body":"Hey Maria. We have not sent a public update in 5+ days. Please send an update to the partner as soon as possible.\n\u0026nbsp;\nReminders:\u0026nbsp;\n  Check to see if the issue is still occurring.  If the issue is resolved, update the partner and mark the ticket as \"Solved\".  \u0026nbsp;\n\u0026nbsp;","created_at":"2024-07-31T03:26:33Z"},{"author_id":15089785937297,"body":"Hello Ken,\n\u0026nbsp;\nI would just like to check on you to see if you have been able to review the information we provided on July 23rd.\n\u0026nbsp;\nThank you,\n\n\n  Maria Eduarda Joazeiro Gomes | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company \n\nUSA: (833) 832 4780 \n\nEMEA: +44 (0) 118 402 9609 \n\nAustralia: +61 (0)2 801 56826 \n\nSingapore: 800-852-3047 \n\nhelpdesk.kaseya.com","created_at":"2024-07-31T03:39:28Z"},{"author_id":375501140238,"body":"Hi Maria: \n \u0026nbsp; \n Thanks for the follow up.\u0026nbsp; Sorry I missed the email from last week.\u0026nbsp; I am going to check with the client and see if they are ok for me to go onsite today.\u0026nbsp; Will get back to you shortly. \n \u0026nbsp; \n \n Thanks. \n \u0026nbsp; \n Kenneth Lam \n \u0026nbsp; Senior Network Specialist \n  \n\n\n\n\n 275 Renfrew Drive \n Suite 105 \n Markham, ON\u0026nbsp;\n \n L3R 0C8 \n\n\n  \n\n\n \u0026nbsp; \n\n\n\n\n P: 905-771-8078 x202 \n F: 905-771-6756 \n E: \nklam@rationalsolutions.com \n To unsubscribe from all future mailing from Rational Business Solutions Inc., please send an email to\nunsubscribe@rationalsolutions.com\n \n\n\n\n\n \u0026nbsp; \n P Please consider our environment\n before printing.\u0026nbsp; \n \u0026nbsp; \n This e-mail may be privileged and/or confidential, and the sender does not waive any related rights and obligations. \n Any distribution, use or copying of this e-mail or the information it contains by other than an intended recipient is unauthorized. \n If you received this e-mail in error, please advise me (by return e-mail or otherwise) immediately \n \u0026nbsp; \n Please be advised that sometimes my SPAM filter eliminates legitimate email.\u0026nbsp; If your email contains important information, please ensure that you\n request acknowledgement of receipt. \n \n \u0026nbsp;","created_at":"2024-07-31T12:36:22Z"},{"author_id":375501140238,"body":"Client confirmed appt for today.\u0026nbsp; I will be on-site with the device in and hour. \n \n\n \n Thanks.\u0026nbsp; \n\nKenneth Lam \n \n \n\n On Jul 31, 2024 8:36 AM, Kenneth Lam \u0026lt;klam@rationalsolutions.com\u0026gt; wrote:\n\n \n \n \n \n Hi Maria: \n \u0026nbsp; \n Thanks for the follow up.\u0026nbsp; Sorry I missed the email from last week.\u0026nbsp; I am going to check with the client and see if they are ok for me to go onsite today.\u0026nbsp; Will get back to you shortly. \n \u0026nbsp; \n \n Thanks. \n \u0026nbsp; \n Kenneth Lam \n \u0026nbsp; Senior Network Specialist \n  \n\n\n\n\n 275 Renfrew Drive \n Suite 105 \n Markham, ON\u0026nbsp;\n \n L3R 0C8 \n\n\n  \n\n\n \u0026nbsp; \n\n\n\n\n P: 905-771-8078 x202 \n F: 905-771-6756 \n E: \nklam@rationalsolutions.com \n To unsubscribe from all future mailing from Rational Business Solutions Inc., please send an email to\nunsubscribe@rationalsolutions.com\n \n\n\n\n\n \u0026nbsp; \n P Please consider our environment before printing.\u0026nbsp; \n \u0026nbsp; \n This e-mail may be privileged and/or confidential, and the sender does not waive any related rights and obligations. \n Any distribution, use or copying of this e-mail or the information it contains by other than an intended recipient is unauthorized. \n If you received this e-mail in error, please advise me (by return e-mail or otherwise) immediately \n \u0026nbsp; \n Please be advised that sometimes my SPAM filter eliminates legitimate email.\u0026nbsp; If your email contains important information, please ensure that you request acknowledgement of\n receipt. \n \n \u0026nbsp;","created_at":"2024-07-31T14:12:14Z"},{"author_id":11408584931985,"body":"Kenneth Lam PAID UFNZ4D","created_at":"2024-07-31T15:42:57Z"},{"author_id":11408584931985,"body":"Below is the teams link\nhttps://teams.microsoft.com/l/meetup-join/19%3ameeting_ODQ3NTE3OTYtMjViMC00YTcyLWIwZDMtZDE1NDgwZDdjYzE5%40thread.v2/0?context=%7b%22Tid%22%3a%22a1cd3436-6062-4169-a1bd-79efdcfd8a5e%22%2c%22Oid%22%3a%22dfd05cb4-2b99-4fea-8641-5737a3424a78%22%7d\n\n\n  Frank Coniglio | Technical Support L2\n\nDatto, a Kaseya Company\n\nUSA: (833) 832 4780\n\nEMEA: +44 (0) 118 402 9609\n\nAustralia: +61 (0)2 801 56826\n\nSingapore: 800-852-3047\n\nhelpdesk.kaseya.com","created_at":"2024-07-31T15:57:53Z"},{"author_id":11408584931985,"body":"Repair on partition","created_at":"2024-07-31T16:18:49Z"},{"author_id":11408584931985,"body":"IBU also clean","created_at":"2024-07-31T16:20:52Z"},{"author_id":11408584931985,"body":"OS was healthy, this Datto hasn't crashed in over a week after crashing almost daily\n fconiglio@Weis-Backup02:~# last\nroot     pts/2        127.0.0.1        Wed Jul 31 11:39    gone - no logout\nstatusus tty1                          Sat Jul 27 10:52   still logged in\nreboot   system boot  5.15.0-105-gener Sat Jul 27 10:52   still running\nroot     pts/2        127.0.0.1        Tue Jul 23 12:51 - 14:04  (01:12)\nroot     pts/4        127.0.0.1        Tue Jul 23 12:31 - 12:51  (00:20)\nroot     pts/2        127.0.0.1        Mon Jul 22 15:59 - 17:10  (01:10)\nroot     pts/2        127.0.0.1        Mon Jul 22 13:52 - 14:51  (00:58)\nstatusus tty1                          Mon Jul 22 12:02 - crash (4+22:50)\nreboot   system boot  5.15.0-105-gener Mon Jul 22 12:02   still running\nstatusus tty1                          Tue Jul 16 14:41 - crash (5+21:20)\nreboot   system boot  5.15.0-105-gener Tue Jul 16 14:41   still running\nstatusus tty1                          Sun Jul 14 13:12 - crash (2+01:28)\nreboot   system boot  5.15.0-105-gener Sun Jul 14 13:12   still running\nstatusus tty1                          Fri Jul 12 07:23 - crash (2+05:49)\nreboot   system boot  5.15.0-105-gener Fri Jul 12 07:22   still running\nstatusus tty1                          Wed Jul 10 10:17 - crash (1+21:05)\nreboot   system boot  5.15.0-105-gener Wed Jul 10 10:17   still running\nstatusus tty1                          Sat Jul  6 08:27 - crash (4+01:50)\nreboot   system boot  5.15.0-105-gener Sat Jul  6 08:27   still running\nstatusus tty1                          Sat Jul  6 05:32 - crash  (02:54)\nreboot   system boot  5.15.0-105-gener Sat Jul  6 05:32   still running \u0026nbsp;\nDatto is on a UPS, partner refused to move datto to a bypassed outlet due to power concerns in this environment.","created_at":"2024-07-31T16:30:11Z"},{"author_id":11408584931985,"body":"Thank you for calling Datto support. \u0026nbsp;Today we ran a repair on the Datto's OS. \u0026nbsp;You will be monitoring this Datto appliance for any further stability events.\u0026nbsp;\n\n\n  Frank Coniglio | Technical Support L2\n\nDatto, a Kaseya Company\n\nUSA: (833) 832 4780\n\nEMEA: +44 (0) 118 402 9609\n\nAustralia: +61 (0)2 801 56826\n\nSingapore: 800-852-3047\n\nhelpdesk.kaseya.com","created_at":"2024-07-31T20:36:47Z"},{"author_id":375501140238,"body":"Hi Frank: \n \u0026nbsp; \n Please help check Weis’ datto device.\u0026nbsp; We just decrypted its agent at 3pm.\u0026nbsp; Seems that device reboot itself at around 2pm today. \n \u0026nbsp; \n \n Thanks. \n \u0026nbsp; \n Kenneth Lam \n \u0026nbsp; Senior Network Specialist \n  \n\n\n\n\n 275 Renfrew Drive \n Suite 105 \n Markham, ON\u0026nbsp;\n \n L3R 0C8 \n\n\n  \n\n\n \u0026nbsp; \n\n\n\n\n P: 905-771-8078 x202 \n F: 905-771-6756 \n E: \nklam@rationalsolutions.com \n To unsubscribe from all future mailing from Rational Business Solutions Inc., please send an email to\nunsubscribe@rationalsolutions.com\n \n\n\n\n\n \u0026nbsp; \n P Please consider our environment\n before printing.\u0026nbsp; \n \u0026nbsp; \n This e-mail may be privileged and/or confidential, and the sender does not waive any related rights and obligations. \n Any distribution, use or copying of this e-mail or the information it contains by other than an intended recipient is unauthorized. \n If you received this e-mail in error, please advise me (by return e-mail or otherwise) immediately \n \u0026nbsp; \n Please be advised that sometimes my SPAM filter eliminates legitimate email.\u0026nbsp; If your email contains important information, please ensure that you\n request acknowledgement of receipt. \n \n \u0026nbsp;","created_at":"2024-08-02T19:10:38Z"},{"author_id":15089785937297,"body":"Hello Ken,\n\u0026nbsp;\nThank you for the update!\nSince we haven't been able to identify the cause of the crashes, we will proceed with an RMA of the device's chassis to prevent further interruptions to the device's functionality.\n\u0026nbsp;\nPlease reply to this email with the shipping information for the new chassis so we can proceed.\n\u0026nbsp;\nThank you,\n\n\n  Maria Eduarda Joazeiro Gomes | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company \n\nUSA: (833) 832 4780 \n\nEMEA: +44 (0) 118 402 9609 \n\nAustralia: +61 (0)2 801 56826 \n\nSingapore: 800-852-3047 \n\nhelpdesk.kaseya.com","created_at":"2024-08-03T00:25:03Z"},{"author_id":15089785937297,"body":"Jean Paul Branca has approved a chassis swap","created_at":"2024-08-03T00:25:18Z"},{"author_id":375501140238,"body":"Hi Maria: \n \u0026nbsp; \n Here is client’s address: \n Attn: Pamela Tiller \n 310 Spadina Avenue, Suite 100A \n Toronto, Ontario \n M5T 2E7 \n Canada \n \u0026nbsp; \n Quick question, I just need to call in for data migration once replacement device is online right? \n \u0026nbsp; \n \n Thanks. \n \u0026nbsp; \n Kenneth Lam \n \u0026nbsp; Senior Network Specialist \n  \n\n\n\n\n 275 Renfrew Drive \n Suite 105 \n Markham, ON\u0026nbsp;\n \n L3R 0C8 \n\n\n  \n\n\n \u0026nbsp; \n\n\n\n\n P: 905-771-8078 x202 \n F: 905-771-6756 \n E: klam@rationalsolutions.com \n To unsubscribe from all future mailing from Rational Business Solutions Inc., please send an email to\nunsubscribe@rationalsolutions.com\n \n\n\n\n\n \u0026nbsp; \n P Please consider our environment\n before printing.\u0026nbsp; \n \u0026nbsp; \n This e-mail may be privileged and/or confidential, and the sender does not waive any related rights and obligations. \n Any distribution, use or copying of this e-mail or the information it contains by other than an intended recipient is unauthorized. \n If you received this e-mail in error, please advise me (by return e-mail or otherwise) immediately \n \u0026nbsp; \n Please be advised that sometimes my SPAM filter eliminates legitimate email.\u0026nbsp; If your email contains important information, please ensure that you\n request acknowledgement of receipt. \n \n \u0026nbsp;","created_at":"2024-08-06T13:11:13Z"},{"author_id":11408303966353,"body":"Hello Ken, \u0026nbsp;\n\u0026nbsp;\nWe have submitted the RMA at this moment, as soon a label gets created, we will send you the tracking information.\u0026nbsp;\n\u0026nbsp;\nThe RMA approved was a \"Chassis\" \u0026nbsp;this will be an empty console or case. \u0026nbsp;The process will be to take the drives from the old chassis to the new one. \u0026nbsp;\n\u0026nbsp;\nThen we will perform a cloud migration due to every chassis have a unique identifier.\n\u0026nbsp;\nYou will need to call us when you go onsite to perform this. \u0026nbsp;\n\u0026nbsp;\nHave a great day!\n\n\n  Jeremy Marrero | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 832-4780\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-06T23:57:51Z"},{"author_id":11408303966353,"body":"Tracking number:\u0026nbsp;\n\u0026nbsp;\n1Z5RR6206673954260\n1Z5RR6209174455296","created_at":"2024-08-07T20:24:52Z"},{"author_id":11408303966353,"body":"Hello Ken, \u0026nbsp;\n\u0026nbsp;\nWe have the tracking numbers for the order. \u0026nbsp;\n\u0026nbsp;\nAt the moment they are not updated on the UPS portal (Due to the label getting created an hour ago), please check in the next 24 hours if there is any information in their portal. \u0026nbsp;\n\u0026nbsp;\nIf there is nothing after, let us know to investigate the label. \u0026nbsp;\n\u0026nbsp;\nTracking numbers are 1Z5RR6209174455296 and 1Z5RR6206673954260\n\u0026nbsp;\nWe will wait for your response. \u0026nbsp;\n\u0026nbsp;\nHave a great day!\u0026nbsp;\n\n\n  Jeremy Marrero | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 832-4780\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-07T20:31:53Z"},{"author_id":375501140238,"body":"Hi Jeremy: \n \u0026nbsp; \n I see that UPS was trying to deliver the chassis today but failed.\u0026nbsp; I checked with the client and they don’t have anyone onsite today and tomorrow.\u0026nbsp; Please help to re-arrange delivery for Monday instead. \n \u0026nbsp; \n \n Thanks. \n \u0026nbsp; \n Kenneth Lam \n \u0026nbsp; Senior Network Specialist \n  \n\n\n\n\n 275 Renfrew Drive \n Suite 105 \n Markham, ON\u0026nbsp;\n \n L3R 0C8 \n\n\n  \n\n\n \u0026nbsp; \n\n\n\n\n P: 905-771-8078 x202 \n F: 905-771-6756 \n E: \nklam@rationalsolutions.com \n To unsubscribe from all future mailing from Rational Business Solutions Inc., please send an email to\nunsubscribe@rationalsolutions.com\n \n\n\n\n\n \u0026nbsp; \n P Please consider our environment\n before printing.\u0026nbsp; \n \u0026nbsp; \n This e-mail may be privileged and/or confidential, and the sender does not waive any related rights and obligations. \n Any distribution, use or copying of this e-mail or the information it contains by other than an intended recipient is unauthorized. \n If you received this e-mail in error, please advise me (by return e-mail or otherwise) immediately \n \u0026nbsp; \n Please be advised that sometimes my SPAM filter eliminates legitimate email.\u0026nbsp; If your email contains important information, please ensure that you\n request acknowledgement of receipt. \n \n \u0026nbsp;","created_at":"2024-08-08T14:42:35Z"},{"author_id":11408303966353,"body":"Hello ken, \u0026nbsp;\n\u0026nbsp;\nThanks for letting us know. \u0026nbsp;\n\u0026nbsp;\nI checked the tracking number 1Z5RR6206673954260 to get information for us to see if we could arrange, but it's showing it got delivered.\u0026nbsp;\n\u0026nbsp;\nCan you confirm this to be true? \u0026nbsp;(Proof of Delivery is tied to this email).\u0026nbsp;\n\u0026nbsp;\nWe will wait for your response. \u0026nbsp;\n\u0026nbsp;\nHave a great day!\u0026nbsp;\n\n\n  Jeremy Marrero | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 832-4780\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-08T21:49:08Z"},{"author_id":375501140238,"body":"Confirmed client receive replacement unit.\u0026nbsp;\u0026nbsp; \n \n\n \n Thanks.\u0026nbsp; \n\nKenneth Lam \n \n \n\n On Aug 8, 2024 5:50 PM, \"Jeremy Marrero (Kaseya Support)\" \u0026lt;support@kaseya.zendesk.com\u0026gt; wrote:\n\n \n \n \n \n \nCAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and know the content is safe.","created_at":"2024-08-09T00:25:51Z"},{"author_id":11408303966353,"body":"Hello Ken, \u0026nbsp;\n\u0026nbsp;\nThanks for letting us know. \u0026nbsp;\n\u0026nbsp;\nWe will wait for your call or email when onsite. \u0026nbsp;\n\u0026nbsp;\nHave a great day!\u0026nbsp;\n\n\n  Jeremy Marrero | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 832-4780\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-09T00:59:35Z"},{"author_id":26547184729233,"body":"sent ken an auth pin via phone ZQXNHD","created_at":"2024-08-13T15:06:21Z"},{"author_id":26547184729233,"body":"Partner is currently onsite and ready for next steps with Chassis he recieved.\u0026nbsp;","created_at":"2024-08-13T15:18:20Z"},{"author_id":26547184729233,"body":"Joe had me transfer the call to L2 tech Johnny Z.\u0026nbsp;","created_at":"2024-08-13T15:23:55Z"},{"author_id":11408459599889,"body":"Old device SN: E43D1AA7924C\u0026nbsp;\nOffsite: server1488\u0026nbsp;\n\u0026nbsp;\nNew device SN: E43D1AA7924C \u0026nbsp;\nOffsite: server6056\u0026nbsp;\n\u0026nbsp;\nAutomated Chassis Swap\u0026nbsp;\nhttps://internal-kb.datto.com/bcdr/Content/bcdr/360038937691.html?Highlight=chassis\u0026nbsp;\n\u0026nbsp;\nLooks like old device is still functional\u0026nbsp;\n\u0026nbsp;\nstep 1\nAccess the old Datto device via RLY\nstep2\nRun to copy the last configuration to the Datto Array drives (/home/configBackup)\n Run to copy the last configuration to the Datto Array drives (/home/configBackup)\u0026nbsp;\n\nsnapctl backup:config step 3\n jzhuo@Weis-Backup02:~# snapctl verification:cleanup\n24-08-13 11:33:54 VER1100 device-general e4c3d76d ...ficationCleanupManager(2):155 DEBU Verification cleanup started\n24-08-13 11:33:54 VER1101 device-general e4c3d76d ...ficationCleanupManager(2):162 DEBU Verification cleanup completed \u0026nbsp;\nPartner went ahead and registered the new chassis.\u0026nbsp;\n\u0026nbsp;\nPowered both devices down and moved over the 2 8TB array drives.\u0026nbsp;\n\u0026nbsp;\nWas able to get the new chassis to check in\u0026nbsp;\n\u0026nbsp;\nRunning auto script\u0026nbsp;\n jzhuo@Weis-Backup03:~# supportctl script autoOSSwap -c\n\n/*==============================================================\n=====================  Datto Chassis Swap  =====================\n==============================================================*/\n\nPlease provide the ServiceCloud Case Number: 5680042\n[WARN]\tAugust 13, 2024 11:53:23 AM: zpool by name of \"homePool\" is not imported.\n[INFO]\tAugust 13, 2024 11:53:38 AM: Taking a backup of existing configuration to ConfigBackup, as well as additional files. Please be patient, this can take some time (up to 10 minutes).\n[INFO]\tAugust 13, 2024 11:54:43 AM: Configuration copied. Please restore from homePool/home/configBackup@1723564476 if necessary.\n\nPlease confirm the date of the last successful checkin from previous Datto appliance from the below list of dates.\n\nNumber:\tDATE\n\n\t0: February 10, 2024 03:54:42 AM\n\t1: February 11, 2024 02:02:01 AM\n\t2: February 12, 2024 05:46:01 AM\n\t3: February 13, 2024 03:23:56 AM\n\t4: February 14, 2024 05:43:19 AM\n\t5: February 15, 2024 03:36:50 AM\n\t6: February 16, 2024 02:42:56 AM\n\t7: February 17, 2024 04:20:12 AM\n\t8: February 18, 2024 04:03:02 AM\n\t9: February 19, 2024 05:02:13 AM\n\t10: February 20, 2024 03:02:41 AM\n\t11: February 21, 2024 03:48:17 AM\n\t12: February 22, 2024 03:46:05 AM\n\t13: February 23, 2024 05:48:45 AM\n\t14: February 24, 2024 04:00:07 AM\n\t15: February 25, 2024 03:54:04 AM\n\t16: February 26, 2024 04:51:53 AM\n\t17: February 27, 2024 05:28:43 AM\n\t18: February 28, 2024 03:15:21 AM\n\t19: February 29, 2024 02:23:08 AM\n\t20: March 01, 2024 04:37:29 AM\n\t21: March 02, 2024 03:26:38 AM\n\t22: March 03, 2024 02:42:29 AM\n\t23: March 04, 2024 04:09:36 AM\n\t24: March 05, 2024 02:33:38 AM\n\t25: March 06, 2024 02:38:31 AM\n\t26: March 07, 2024 05:33:40 AM\n\t27: March 08, 2024 04:38:48 AM\n\t28: March 09, 2024 03:08:29 AM\n\t29: March 11, 2024 05:31:27 AM\n\t30: March 12, 2024 02:24:33 AM\n\t31: March 13, 2024 05:03:06 AM\n\t32: March 14, 2024 02:27:41 AM\n\t33: March 15, 2024 04:04:20 AM\n\t34: March 16, 2024 04:56:40 AM\n\t35: March 17, 2024 05:37:52 AM\n\t36: March 18, 2024 03:34:05 AM\n\t37: March 19, 2024 04:06:56 AM\n\t38: March 20, 2024 02:53:20 AM\n\t39: March 21, 2024 03:32:28 AM\n\t40: March 22, 2024 03:07:06 AM\n\t41: March 23, 2024 05:31:32 AM\n\t42: March 24, 2024 04:26:05 AM\n\t43: March 25, 2024 05:18:31 AM\n\t44: March 26, 2024 02:15:05 AM\n\t45: March 27, 2024 05:38:45 AM\n\t46: March 28, 2024 04:55:54 AM\n\t47: March 29, 2024 04:14:52 AM\n\t48: March 30, 2024 05:59:12 AM\n\t49: March 31, 2024 03:23:39 AM\n\t50: April 01, 2024 04:22:16 AM\n\t51: April 02, 2024 03:06:27 AM\n\t52: April 03, 2024 02:14:29 AM\n\t53: April 04, 2024 02:56:07 AM\n\t54: April 05, 2024 05:34:45 AM\n\t55: April 06, 2024 03:40:42 AM\n\t56: April 07, 2024 03:39:05 AM\n\t57: April 08, 2024 05:46:01 AM\n\t58: April 09, 2024 04:05:20 AM\n\t59: April 10, 2024 02:52:30 AM\n\t60: April 11, 2024 04:14:58 AM\n\t61: April 12, 2024 04:01:46 AM\n\t62: April 13, 2024 03:08:15 AM\n\t63: April 14, 2024 02:21:04 AM\n\t64: April 15, 2024 04:00:43 AM\n\t65: April 16, 2024 05:22:10 AM\n\t66: April 17, 2024 05:15:24 AM\n\t67: April 18, 2024 04:55:16 AM\n\t68: April 19, 2024 03:38:14 AM\n\t69: April 20, 2024 04:59:49 AM\n\t70: April 21, 2024 02:33:17 AM\n\t71: April 22, 2024 05:00:05 AM\n\t72: April 23, 2024 05:43:06 AM\n\t73: April 24, 2024 02:43:04 AM\n\t74: April 25, 2024 03:04:43 AM\n\t75: April 26, 2024 04:11:06 AM\n\t76: April 27, 2024 03:47:32 AM\n\t77: April 28, 2024 02:10:05 AM\n\t78: April 29, 2024 05:33:05 AM\n\t79: April 30, 2024 02:20:16 AM\n\t80: May 01, 2024 05:33:03 AM\n\t81: May 02, 2024 05:36:20 AM\n\t82: May 03, 2024 02:03:18 AM\n\t83: May 04, 2024 03:30:48 AM\n\t84: May 05, 2024 03:15:32 AM\n\t85: May 06, 2024 04:12:40 AM\n\t86: May 07, 2024 02:31:06 AM\n\t87: May 08, 2024 04:37:25 AM\n\t88: May 09, 2024 03:32:45 AM\n\t89: May 10, 2024 05:23:43 AM\n\t90: May 11, 2024 05:20:43 AM\n\t91: May 12, 2024 04:50:22 AM\n\t92: May 13, 2024 05:56:51 AM\n\t93: May 14, 2024 02:20:35 AM\n\t94: May 15, 2024 02:57:59 AM\n\t95: May 16, 2024 03:27:33 AM\n\t96: May 17, 2024 02:28:46 AM\n\t97: May 18, 2024 05:10:16 AM\n\t98: May 19, 2024 02:15:34 AM\n\t99: May 20, 2024 05:28:31 AM\n\t100: May 22, 2024 02:47:28 AM\n\t101: May 23, 2024 05:16:28 AM\n\t102: May 24, 2024 05:07:36 AM\n\t103: May 25, 2024 02:02:08 AM\n\t104: May 26, 2024 02:40:27 AM\n\t105: May 27, 2024 03:28:52 AM\n\t106: May 28, 2024 04:08:41 AM\n\t107: May 29, 2024 05:01:00 AM\n\t108: May 30, 2024 02:16:03 AM\n\t109: May 31, 2024 02:02:54 AM\n\t110: June 01, 2024 05:33:05 AM\n\t111: June 02, 2024 05:08:51 AM\n\t112: June 03, 2024 03:25:00 AM\n\t113: June 04, 2024 05:59:42 AM\n\t114: June 05, 2024 02:41:27 AM\n\t115: June 06, 2024 03:11:27 AM\n\t116: June 07, 2024 05:06:06 AM\n\t117: June 08, 2024 05:45:06 AM\n\t118: June 09, 2024 05:46:51 AM\n\t119: June 10, 2024 05:07:15 AM\n\t120: June 11, 2024 04:43:34 AM\n\t121: June 12, 2024 04:18:43 AM\n\t122: June 13, 2024 03:19:22 AM\n\t123: June 14, 2024 05:15:57 AM\n\t124: June 15, 2024 02:41:54 AM\n\t125: June 16, 2024 04:14:16 AM\n\t126: June 17, 2024 04:10:35 AM\n\t127: June 18, 2024 05:53:41 AM\n\t128: June 19, 2024 02:55:27 AM\n\t129: June 20, 2024 02:36:16 AM\n\t130: June 21, 2024 03:54:25 AM\n\t131: June 22, 2024 03:51:56 AM\n\t132: June 23, 2024 05:08:04 AM\n\t133: June 24, 2024 05:30:10 AM\n\t134: June 25, 2024 02:56:08 AM\n\t135: June 26, 2024 05:03:28 AM\n\t136: June 27, 2024 03:40:46 AM\n\t137: June 28, 2024 05:23:46 AM\n\t138: June 29, 2024 03:36:24 AM\n\t139: June 30, 2024 04:21:43 AM\n\t140: July 01, 2024 02:23:41 AM\n\t141: July 02, 2024 02:32:10 AM\n\t142: July 03, 2024 03:51:20 AM\n\t143: July 04, 2024 04:59:29 AM\n\t144: July 05, 2024 02:39:43 AM\n\t145: July 07, 2024 02:31:43 AM\n\t146: July 08, 2024 02:44:10 AM\n\t147: July 09, 2024 02:33:59 AM\n\t148: July 10, 2024 03:13:18 AM\n\t149: July 11, 2024 02:54:44 AM\n\t150: July 12, 2024 02:17:06 AM\n\t151: July 13, 2024 04:07:34 AM\n\t152: July 14, 2024 03:49:32 AM\n\t153: July 15, 2024 04:58:32 AM\n\t154: July 16, 2024 02:29:29 AM\n\t155: July 17, 2024 03:57:31 AM\n\t156: July 18, 2024 04:04:08 AM\n\t157: July 19, 2024 05:17:56 AM\n\t158: July 20, 2024 02:47:24 AM\n\t159: July 21, 2024 02:18:53 AM\n\t160: July 22, 2024 05:29:42 AM\n\t161: July 23, 2024 02:47:47 AM\n\t162: July 24, 2024 03:50:15 AM\n\t163: July 25, 2024 03:15:54 AM\n\t164: July 26, 2024 05:18:23 AM\n\t165: July 27, 2024 02:21:31 AM\n\t166: July 28, 2024 03:38:33 AM\n\t167: July 29, 2024 04:09:47 AM\n\t168: July 30, 2024 04:31:04 AM\n\t169: July 31, 2024 03:34:59 AM\n\t170: August 01, 2024 02:00:33 AM\n\t171: August 02, 2024 02:54:31 AM\n\t172: August 03, 2024 04:31:11 AM\n\t173: August 04, 2024 02:17:27 AM\n\t174: August 05, 2024 05:56:38 AM\n\t175: August 06, 2024 05:54:06 AM\n\t176: August 07, 2024 02:10:04 AM\n\t177: August 08, 2024 05:03:34 AM\n\t178: August 09, 2024 02:52:39 AM\n\t179: August 10, 2024 04:27:07 AM\n\t180: August 11, 2024 02:46:42 AM\n\t181: August 12, 2024 04:12:20 AM\n\t182: August 13, 2024 05:54:00 AM\n\t183: August 13, 2024 11:32:48 AM\n\nPlease confirm the date of the last successful checkin from previous Datto appliance from the above list of dates.\n\n\nType the NUMBER next to the date of the last successful checkin.\n\nPlease enter a number. Press [CTRL] + C to quit.\n\n\nEnter Selection: 183\n\n[WARN]\tAugust 13, 2024 11:55:17 AM: The partner will need to configure their networking again via the console. Ask your Datto Tech Support expert for guidance on this.\ncannot open 'homePool/home/owncloud': dataset does not exist\n[WARN]\tAugust 13, 2024 11:55:20 AM: Could not copy the user, \"rational\" as an existing user by the same name exists in NEW OS User File (passwd).\n[INFO]\tAugust 13, 2024 11:55:32 AM: Repairing communication with paired agent / agentless systems. This may take some time depending on the number of agents.\n[INFO]\tAugust 13, 2024 11:55:32 AM: Performing offsite check and refreshing recovery points.\n[ERROR]\tAugust 13, 2024 11:55:32 AM: The following errors occurred during the copy operations. Please review with your Datto Technical Support team before continuing:\n\n|=======================  Error  Log  ========================|\n\n[WARN]\tAugust 13, 2024 11:53:23 AM: zpool by name of \"homePool\" is not imported.\n[WARN]\tAugust 13, 2024 11:55:17 AM: The partner will need to configure their networking again via the console. Ask your Datto Tech Support expert for guidance on this.\n[WARN]\tAugust 13, 2024 11:55:20 AM: Could not copy the user, \"rational\" as an existing user by the same name exists in NEW OS User File (passwd).\n\n|=============================================================|\n\nPress [ENTER] to continue.\n\nINFORMATION FOR OUR PARTNER\n\t+ Maintenance mode has been disabled.\n\n\t+ You must ensure that Timezone information is correct in Configuration -\u0026gt; Device Settings. Failure to do so will\n\t  result in potential data loss. \n\n\t+ You will need to set the networking configuration of this Datto via the console, or by navigating to the\n\t  Web UI. Please ask a Datto Technical Support Expert to assist you with this if required.\n\n\nINFORMATION FOR DATTO TECH SUPPORT\n\t+ You must ensure that Timezone information is correct in Configuration -\u0026gt; Device Settings. Failure to do so will\n\t  result in potential data loss.\n\n\t+ You must rotate the BackupAdmin Password using the Datto Admin page.\n\n\t+ Please submit an Offsite Reassociation request in the ServiceCloud Case, to ensure that the Chassis is \n\t  linked to the correct Offsite Datasets. Offsite transfer will be paused.\n\n24-08-13 11:55:38 SSM0003 device-general 3f008ce9 ...SyncMaintenanceService(2):195 INFO Offsite replication paused by device-web\n24-08-13 11:55:38 CUR0001 device-general 3f008ce9 ...ice/src/Curl/CurlHelper(2):89 INFO CurlHelper Request\n                                                                                   CTXT method: GET\n                                                                                        url: https://device.dattobackup.com/setDelay.php?key=8bfadf8673af695878d6071e1ed7e25f\u0026amp;delay=87600\n24-08-13 11:55:38 CUR0002 device-general 3f008ce9 ...ce/src/Curl/CurlHelper(2):102 INFO CurlHelper Response\n                                                                                   CTXT method: GET\n                                                                                        url: https://device.dattobackup.com/setDelay.php?key=8bfadf8673af695878d6071e1ed7e25f\u0026amp;delay=87600\n                                                                                        response: \n                                                                                          \u0026lt;html\u0026gt;\n                                                                                              \u0026lt;head\u0026gt;\n                                                                                                  \u0026lt;META HTTP-EQUIV=\"Refresh\"\n                                                                                                        CONTENT=\"0; URL=http://192.168.2.176/includes/setDelay.php?success=true\u0026amp;delay=87600\"\u0026gt;\n                                                                                              \u0026lt;/head\u0026gt;\n                                                                                              \u0026lt;body\u0026gt;\n                                                                                                  \u0026lt;h1\u0026gt;:87600:Redirecting From the Server\n                                                                                                  \u0026lt;/h1\u0026gt;\n                                                                                              \u0026lt;/body\u0026gt;\n                                                                                          \u0026lt;/html\u0026gt;\n\nDatto Appliance is ready to be rebooted. \n\n\nThanks for choosing Datto! \u0026nbsp;\nrunning into slog drive error missing\u0026nbsp;\n jzhuo@Weis-Backup03:~# zpool status -v\n  pool: homePool\n state: DEGRADED\nstatus: One or more devices could not be used because the label is missing or\n\tinvalid.  Sufficient replicas exist for the pool to continue\n\tfunctioning in a degraded state.\naction: Replace the device using 'zpool replace'.\n   see: https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-4J\n  scan: resilvered 0B in 00:01:49 with 0 errors on Mon Nov  6 12:24:27 2023\nconfig:\n\n\tNAME                        STATE     READ WRITE CKSUM\n\thomePool                    DEGRADED     0     0     0\n\t  mirror-0                  ONLINE       0     0     0\n\t    wwn-0x5000c500da104987  ONLINE       0     0     0\n\t    scsi-35000cca0c266a79c  ONLINE       0     0     0\n\tlogs\t\n\t  17598736664264038066      UNAVAIL      0     0     0  was /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0T501177-part1\n\nerrors: No known data errors \u0026nbsp;\n\n\nIt looks like sdb is supoose to be the slog drive\u0026nbsp;\n jzhuo@Weis-Backup03:~# lsscsi -s\n[0:0:0:0]    disk    SEAGATE  ST8000NM014A     CSLC  /dev/sda   8.00TB\n[0:0:1:0]    disk    ATA      MZ7KH480HAHQ0D3  HF58  /dev/sdb    480GB\n[0:0:2:0]    disk    HGST     HUS728T8TAL5200  RS06  /dev/sdc   8.00TB\n[15:0:0:0]   disk    ATA      SSDSCKKB240GZR   DL74  /dev/sdd    240GB\n[16:0:0:0]   disk    ATA      SSDSCKKB240GZR   DL74  /dev/sde    240GB\n[17:0:0:0]   process Marvell  Console          1.01  -               - Getting error\u0026nbsp;\n jzhuo@Weis-Backup03:~# zpool replace homePool /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0T501177-part1 /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0R306959\n/dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0R306959 is in use and contains a unknown filesystem. But it has stone ship partition on it ....\u0026nbsp;\n jzhuo@Weis-Backup03:~# blkid | grep -i stoneship\n/dev/sdb4: LABEL=\"Stoneship\" UUID=\"095574f9-0ebd-4a96-8edb-cfeaafd8c262\" TYPE=\"ext4\" PARTLABEL=\"Stoneship\" PARTUUID=\"3e73e1e7-be26-4c2c-b3be-1cdb671737f6\" \u0026nbsp;\nPt: 5708398 Datto BCDR (S5-6 to 36 TB) Chassis replacements coming from build with the SLOG drive imaged as OS\u0026nbsp;\n\u0026nbsp;\nNeed to replace the whole chassis once more.....\u0026nbsp;\n\u0026nbsp;\nWill need to send them another chassis\n\u0026nbsp;\nFor now partner can use the new chassis to perform backups and etc.\u0026nbsp;\n\u0026nbsp;\nStill going to move over the cloud data to the new chassis then when partner gets another one move it over to the new new chassis.\u0026nbsp;","created_at":"2024-08-13T16:15:13Z"},{"author_id":11408459599889,"body":"NEXT STEPS SO THERE IS NO CONFUSION.\n\u0026nbsp;\nSending them a new chassis per PT as the one they received was not images correctly and is missing a slog drive.\u0026nbsp;\n\u0026nbsp;\nPartner is still able to perform backups and etc... but no local virt (Explained this to the partner)\u0026nbsp;\n\u0026nbsp;\nOnce partner received the new new chassis we would perform another chassis swap.\u0026nbsp;\n\u0026nbsp;\nFor the one they have right now (84160C2930AA) still going to move the cloud data to this one\u0026nbsp;\n\u0026nbsp;\nFinish off the cloud reassoiction we submitted.\u0026nbsp;\n\u0026nbsp;\nOnce the partner gets the new new chassis we would perform chassis swap and cloud from 84160C2930AA \u0026gt; new new chassis\u0026nbsp;","created_at":"2024-08-13T16:22:39Z"},{"author_id":11408459599889,"body":"Hello Ken,\u0026nbsp;\n\u0026nbsp;\nThank you for calling in today, as on the phone we were able to perform the chassis swap. However we found out that the chassis we sent out was not imaged correctly. As the new chassis is still able to perfrom backups, please continue to backup with this new chassis. We are moving over the cloud data from the old chassis to this one as well. Please keep offsite sync paused in the meantime.\u0026nbsp;\n\u0026nbsp;\nWe are in the porcess of sending you another chassis that is correctly imaged. Once you receive this please give us a call as we would need to perform another chassis swap.\u0026nbsp;\n\u0026nbsp;\nLet us know if you have any questions in the meantime. We will keep you updated on the cloud migration as well as a new tracking number.\u0026nbsp;\n\u0026nbsp;\nThank you,\u0026nbsp;\n\n\n  Johnny Zhuo | Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nUSA: (833) 832 4780\n\nEMEA: +44 (0) 118 402 9609\n\nAustralia: +61 (0)2 801 56826\n\nSingapore: 800-852-3047\n\nhelpdesk.kaseya.com","created_at":"2024-08-13T16:28:45Z"},{"author_id":11466564294545,"body":"SUCCESS! PIN: Z22HCP - authed Ken Lam \u0026nbsp; -- klam@rationalsolutions.com\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\nPartner changed IP address to static \u0026amp; is now unable to connect to the Datto. Datto shows the IP \u0026nbsp;192.168.2.178\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;Neither s/n in the ticket is reachable.\u0026nbsp;\n mcicero\u0026gt; c 84160C2930AA\nLooking up client ... Found: device-84160c2930aa Weis-Backup03 393451\nChecking for client ... Not active. Connection will likely fail.\nChecking for existing connection ... xpayqvi5olffc93b (via dlt-rly-relay-5.datto.com)\nWaiting for connection .........^C.\nmcicero\u0026gt; c E43D1AA7924C\nLooking up client ... Found: device-e43d1aa7924c Weis-Backup02 343238\nChecking for client ... Not active. Connection will likely fail.\nChecking for existing connection ... dki857yvmuds0jqk (via dlt-rly-relay-4.datto.com)\nWaiting for connection ......... \u0026nbsp;","created_at":"2024-08-13T16:49:18Z"},{"author_id":11466564294545,"body":"","created_at":"2024-08-13T17:02:55Z"},{"author_id":11466564294545,"body":"Found it. Looks like the Datto got a new IP address, however, unable to connect to it.\u0026nbsp;\n\u0026nbsp;\n176 was the previous IP when the partner tried setting this up.\u0026nbsp;\nPartner did network scan, it showed it on \u0026nbsp;.176 instead of .178 ... \u0026nbsp;but he said that .176 is what it was originally on...","created_at":"2024-08-13T17:07:03Z"},{"author_id":11466564294545,"body":"mcicero@Weis-Backup03:~# zfs list\nno datasets available\nmcicero@Weis-Backup03:~# zpool status -v\nno pools available\nmcicero@Weis-Backup03:~# lsscsi -i -u -s\n[0:0:0:0]    disk    5000c500da104987                  /dev/sda   35000c500da104987  8.00TB\n[0:0:1:0]    disk    5002538e0135761c                  /dev/sdb   -   480GB\n[0:0:2:0]    disk    5000cca0c266a79c                  /dev/sdc   35000cca0c266a79c  8.00TB\n[15:0:0:0]   disk    55cd2e415500f624                  /dev/sdd   -   240GB\n[16:0:0:0]   disk    55cd2e41562bd19f                  /dev/sde   -   240GB\n[17:0:0:0]   process none                              -          -       -\nmcicero@Weis-Backup03:~#  \u0026nbsp;\nOhh look, we have a \"corrupted\" drive. Lets try to import the pool anyways.\u0026nbsp;\n mcicero@Weis-Backup03:~# zpool import homePool \nThe devices below are missing or corrupted, use '-m' to import the pool anyway:\n\t    ata-MZ7KH480HAHQ0D3_S5CNNA0T501177 [log]\n\ncannot import 'homePool': one or more devices is currently unavailable\nmcicero@Weis-Backup03:~#  Got some stuff showing up:\n mcicero@Weis-Backup03:~# zpool status -v\n  pool: homePool\n state: DEGRADED\nstatus: One or more devices could not be used because the label is missing or\n\tinvalid.  Sufficient replicas exist for the pool to continue\n\tfunctioning in a degraded state.\naction: Replace the device using 'zpool replace'.\n   see: https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-4J\n  scan: resilvered 0B in 00:01:49 with 0 errors on Mon Nov  6 12:24:27 2023\nconfig:\n\n\tNAME                        STATE     READ WRITE CKSUM\n\thomePool                    DEGRADED     0     0     0\n\t  mirror-0                  ONLINE       0     0     0\n\t    wwn-0x5000c500da104987  ONLINE       0     0     0\n\t    scsi-35000cca0c266a79c  ONLINE       0     0     0\n\tlogs\t\n\t  17598736664264038066      UNAVAIL      0     0     0  was /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0T501177-part1\n\nerrors: No known data errors\nmcicero@Weis-Backup03:~#","created_at":"2024-08-13T17:21:54Z"},{"author_id":11466564294545,"body":"mcicero@Weis-Backup03:~# lsscsi -i -u -s\n[0:0:0:0]    disk    5000c500da104987                  /dev/sda   35000c500da104987  8.00TB\n[0:0:1:0]    disk    5002538e0135761c                  /dev/sdb   -   480GB\n[0:0:2:0]    disk    5000cca0c266a79c                  /dev/sdc   35000cca0c266a79c  8.00TB\n[15:0:0:0]   disk    55cd2e415500f624                  /dev/sdd   -   240GB\n[16:0:0:0]   disk    55cd2e41562bd19f                  /dev/sde   -   240GB\n[17:0:0:0]   process none                              -          -       -\nmcicero@Weis-Backup03:~# zpool replace homePool /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0T501177-part1 5002538e0135761c\ncannot open '5002538e0135761c': no such device in /dev\nmust be a full path or shorthand device name\nmcicero@Weis-Backup03:~# zpool replace homePool /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0T501177-part1 wwn-0x5002538e0135761c\n/dev/disk/by-id/wwn-0x5002538e0135761c is in use and contains a unknown filesystem.\nmcicero@Weis-Backup03:~# zpool replace homePool /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0T501177-part1 wwn-0x5002538e0135761c \u0026nbsp;","created_at":"2024-08-13T17:30:55Z"},{"author_id":11466564294545,"body":"So it IS PT: 5708398... this ticket has come full \u0026nbsp;circle.\u0026nbsp;","created_at":"2024-08-13T17:49:16Z"},{"author_id":11466564294545,"body":"mcicero@Weis-Backup03:~# zpool status -v\nno pools available\nmcicero@Weis-Backup03:~#   zpool import\n   pool: homePool\n     id: 4840822660934899508\n  state: UNAVAIL\nstatus: One or more devices are missing from the system.\n action: The pool cannot be imported. Attach the missing\n\tdevices and try again.\n   see: https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-6X\n config:\n\n\thomePool                    UNAVAIL  missing device\n\t  mirror-0                  ONLINE\n\t    wwn-0x5000c500da104987  ONLINE\n\t    scsi-35000cca0c266a79c  ONLINE\n\tlogs\t\n\t  17598736664264038066      UNAVAIL\n\n\tAdditional devices are known to be part of this pool, though their\n\texact configuration cannot be determined.\nmcicero@Weis-Backup03:~# zpool status -v\nno pools available\nmcicero@Weis-Backup03:~#","created_at":"2024-08-13T17:54:52Z"},{"author_id":11408459599889,"body":"Cloud is done running the block commands\u0026nbsp;\n\u0026nbsp;\nspeedsync looks good\u0026nbsp;\n \njzhuo@Weis-Backup03:~# speedsync status\n+----------------------------+------------+------------+--------+-------+---------+---------+-------+-------+\n| dataset                    | target     | timestamp  | action | size  | sent    | time    | rate  | state |\n+----------------------------+------------+------------+--------+-------+---------+---------+-------+-------+\n| homePool/home/configBackup | server1488 | 1723564476 | stream | 74 MB | 2.15 MB | 0:00:03 | 0 B/s | Good  |\n+----------------------------+------------+------------+--------+-------+---------+---------+-------+-------+\n\n+-------------------------------------------------------+------------+-----------+--------------+------------+----------+--------+\n| dataset                                               | target     | local zfs | remote files | remote zfs | priority | paused |\n+-------------------------------------------------------+------------+-----------+--------------+------------+----------+--------+\n| homePool/home/agents/b71c75516d1b4c06833aeb140bbf8317 | server1488 | 0         | 0            | 53         | medium   |        |\n| homePool/home/configBackup                            | server1488 | 1         | 0            | 545        | medium   |        |\n| homePool/home/agents/f898177883474688872d40412eb01a1f | server1488 | 0         | 0            | 53         | medium   |        |\n+-------------------------------------------------------+------------+-----------+--------------+------------+----------+--------+ \u0026nbsp;\nstill waiting for tracking number when the new chassis gets sent out.\u0026nbsp;","created_at":"2024-08-13T18:12:24Z"},{"author_id":11408459599889,"body":"Hello Ken,\u0026nbsp;\n\u0026nbsp;\nJust wanted to give you an update on the cloud migration. This has been performed successfully.\u0026nbsp;\n\u0026nbsp;\nWe have gone ahead and resume the offsite sync on the device as well.\u0026nbsp;\n\u0026nbsp;\nWe will update you once we get a tracking number for the new chassis as soon as possible.\u0026nbsp;\n\u0026nbsp;\nThank you,\u0026nbsp;\n\n\n  Johnny Zhuo | Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nUSA: (833) 832 4780\n\nEMEA: +44 (0) 118 402 9609\n\nAustralia: +61 (0)2 801 56826\n\nSingapore: 800-852-3047\n\nhelpdesk.kaseya.com","created_at":"2024-08-13T18:13:33Z"},{"author_id":11466564294545,"body":"partner trying to run a backup now.\u0026nbsp;\n\u0026nbsp;\n                                                                                           #19 {main}\n24-08-13 14:13:34 BAK2000        WEISSRV b474c01e ...ages/TransferAgentData(2):691 DEBU Transferring data, 499933184/7356227584 [ 6 ]   Timeout Countdown: 480-900\n24-08-13 14:14:06 RTU0003        WEISSRV b474c01e ...e/src/Util/RetryHandler(2):57 WARN Failure executing function\n                                                                                   CTXT error: CURL Error: https://192.168.2.227:25568/backup/77dd9e80-7af4-41b4-8f36-36b168d16368: Connection timed out after 30001 milliseconds\n24-08-13 14:14:06 RTU0005        WEISSRV b474c01e ...e/src/Util/RetryHandler(2):62 WARN Reattempting function after waiting\n                                                                                   CTXT secondsAfterAttempt: 2\n24-08-13 14:14:38 RTU0003        WEISSRV b474c01e ...e/src/Util/RetryHandler(2):57 WARN Failure executing function\n                                                                                   CTXT error: CURL Error: https://192.168.2.227:25568/backup/77dd9e80-7af4-41b4-8f36-36b168d16368: Connection timed out after 30000 milliseconds\n24-08-13 14:14:38 RTU0005        WEISSRV b474c01e ...e/src/Util/RetryHandler(2):62 WARN Reattempting function after waiting\n                                                                                   CTXT secondsAfterAttempt: 2","created_at":"2024-08-13T18:18:13Z"},{"author_id":11466564294545,"body":"24-08-13 14:15:10 RTU0004        WEISSRV b474c01e ...e/src/Util/RetryHandler(2):72 ERRO Retry attempts exhausted.\n                                                                                   CTXT error: Function execution attempts exhausted: 0 =\u0026gt; 'CURL Error: https://192.168.2.227:25568/backup/77dd9e80-7af4-41b4-8f36-36b168d16368: Connection timed out after 30001 milliseconds', 1 =\u0026gt; 'CURL Error: https://192.168.2.227:25568/backup/77dd9e80-7af4-41b4-8f36-36b168d16368: Connection timed out after 30000 milliseconds', 2 =\u0026gt; 'CURL Error: https://192.168.2.227:25568/backup/77dd9e80-7af4-41b4-8f36-36b168d16368: Connection timed out after 30001 milliseconds'\n24-08-13 14:15:10 DAA1004        WEISSRV b474c01e ...gent/Api/DattoAgentApi(2):265 ERRO Datto Agent API jobStatus request failed\n                                                                                   CTXT error: Function execution attempts exhausted: 0 =\u0026gt; 'CURL Error: https://192.168.2.227:25568/backup/77dd9e80-7af4-41b4-8f36-36b168d16368: Connection timed out after 30001 milliseconds', 1 =\u0026gt; 'CURL Error: https://192.168.2.227:25568/backup/77dd9e80-7af4-41b4-8f36-36b168d16368: Connection timed out after 30000 milliseconds', 2 =\u0026gt; 'CURL Error: https://192.168.2.227:25568/backup/77dd9e80-7af4-41b4-8f36-36b168d16368: Connection timed out after 30001 milliseconds'\n                                                                                        errorCode: 28 ^ Still trying to run the backup.\u0026nbsp;","created_at":"2024-08-13T18:18:50Z"},{"author_id":11466564294545,"body":"Backup Error at 2:01pm Tuesday 13th August. VSS Export error occurred mid-transfer on previous attempt; falling back to DBD\n\u0026nbsp;\nBackup on one server failed\u0026nbsp;\n\u0026nbsp;\nAssistance from Johnny Zhou\u0026nbsp;\n  \njzhuo@Weis-Backup03:~# nmap -p25568  192.168.2.227\nStarting Nmap 7.80 ( https://nmap.org ) at 2024-08-13 14:33 EDT\nNmap scan report for 192.168.2.227\nHost is up (0.00054s latency).\nPORT      STATE    SERVICE\n25568/tcp filtered unknown\nMAC Address: 00:15:5D:0A:A7:01 (Microsoft)\nNmap done: 1 IP address (1 host up) scanned in 0.47 seconds\n \n  \u0026nbsp;","created_at":"2024-08-13T18:36:27Z"},{"author_id":11466564294545,"body":"Hi Ken,\u0026nbsp;\n\u0026nbsp;\nThanks for contacting Datto BCDR tech support today! At this time, we are seeing your Datto device back online, connected \u0026amp; running backups for both your servers. The issues we saw previously are caused by the Datto having the OS image on the SLOG drive. Please reach out again once the new RMA is received \u0026amp; we can walk you through the next steps with that. If you have any further questions, let us know!\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\nAll the best,\n\n\n  Mike Cicero | Technical Support\n\nDatto, a Kaseya Company\n\nUSA: (833) 832 4780\n\nEMEA: +44 (0) 118 402 9609\n\nAustralia: +61 (0)2 801 56826\n\nSingapore: 800-852-3047\n\nhelpdesk.kaseya.com","created_at":"2024-08-13T19:30:06Z"},{"author_id":375501140238,"body":"Hi Mike: \n \u0026nbsp; \n Client’s datto is still rebooting itself despite chassis replacement.\u0026nbsp; Not sure if it’s because original issue wasn’t related to hardware, or the incorrect image on the current chassis.\u0026nbsp;\n \n Volume is unmounted at the meantime, please help mount it so we can take backup over the weekend. \n  \n Please help check if device rebooted for the same reason as before. \n \u0026nbsp; \n \n Thanks. \n \u0026nbsp; \n Kenneth Lam \n \u0026nbsp; Senior Network Specialist \n  \n\n\n\n\n 275 Renfrew Drive \n Suite 105 \n Markham, ON\u0026nbsp;\n \n L3R 0C8 \n\n\n  \n\n\n \u0026nbsp; \n\n\n\n\n P: 905-771-8078 x202 \n F: 905-771-6756 \n E: klam@rationalsolutions.com \n To unsubscribe from all future mailing from Rational Business Solutions Inc., please send an email to\nunsubscribe@rationalsolutions.com\n \n\n\n\n\n \u0026nbsp; \n P Please consider our environment\n before printing.\u0026nbsp; \n \u0026nbsp; \n This e-mail may be privileged and/or confidential, and the sender does not waive any related rights and obligations. \n Any distribution, use or copying of this e-mail or the information it contains by other than an intended recipient is unauthorized. \n If you received this e-mail in error, please advise me (by return e-mail or otherwise) immediately \n \u0026nbsp; \n Please be advised that sometimes my SPAM filter eliminates legitimate email.\u0026nbsp; If your email contains important information, please ensure that you\n request acknowledgement of receipt. \n \n \u0026nbsp;","created_at":"2024-08-17T17:03:02Z"},{"author_id":11408303966353,"body":"We are in the same spot:\u0026nbsp;\n\u0026nbsp;\n jmarrero@Weis-Backup03:~# zpool status -v\n  pool: homePool\n state: DEGRADED\nstatus: One or more devices could not be used because the label is missing or\n        invalid.  Sufficient replicas exist for the pool to continue\n        functioning in a degraded state.\naction: Replace the device using 'zpool replace'.\n   see: https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-4J\n  scan: resilvered 0B in 00:01:49 with 0 errors on Mon Nov  6 12:24:27 2023\nconfig:\n\n        NAME                        STATE     READ WRITE CKSUM\n        homePool                    DEGRADED     0     0     0\n          mirror-0                  ONLINE       0     0     0\n            wwn-0x5000c500da104987  ONLINE       0     0     0\n            scsi-35000cca0c266a79c  ONLINE       0     0     0\n        logs\n          17598736664264038066      UNAVAIL      0     0     0  was /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0T501177-part1\n\nerrors: No known data errors\njmarrero@Weis-Backup03:~#","created_at":"2024-08-18T21:21:30Z"},{"author_id":11408303966353,"body":"Hello Ken, \u0026nbsp;\n\u0026nbsp;\nThanks for the information. \u0026nbsp;\n\u0026nbsp;\nI have remounted the pool for backups to take place. \u0026nbsp;Please proceed with decrypting the agents. \u0026nbsp;\n\u0026nbsp;\nRegarding the drives, the issue looks like the same scenario. \u0026nbsp;\n\u0026nbsp;\nAfter in investigation, I will update with next steps. \u0026nbsp;\n\u0026nbsp;\nHave a great day!\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\n\n  Jeremy Marrero | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 832-4780\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-18T21:33:15Z"},{"author_id":11408303966353,"body":"Hello Ken, \u0026nbsp;\n\u0026nbsp;\nThe only process I can see we can do moving forward is to send a complete RMA device. \u0026nbsp;\n\u0026nbsp;\nDue to the situation on the old device, this one will need to be pre-seeded to make sure it has all the data we have in the cloud. \u0026nbsp;\n\u0026nbsp;\nThis can add more time to the RMA process. \u0026nbsp;\n\u0026nbsp;\nLet us know your decision on this. \u0026nbsp;\n\u0026nbsp;\nHave a great day!\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\n\n  Jeremy Marrero | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 832-4780\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-20T04:28:45Z"},{"author_id":375501140238,"body":"Hi Jeremy: \n \u0026nbsp; \n Please send complete RMA device. \n We have two chassis waiting to be return in this case(one unopened, arrived Monday).\u0026nbsp; Should we send them back first? \n \u0026nbsp; \n \n Thanks. \n \u0026nbsp; \n Kenneth Lam \n \u0026nbsp; Senior Network Specialist \n  \n\n\n\n\n 275 Renfrew Drive \n Suite 105 \n Markham, ON\u0026nbsp;\n \n L3R 0C8 \n\n\n  \n\n\n \u0026nbsp; \n\n\n\n\n P: 905-771-8078 x202 \n F: 905-771-6756 \n E: \nklam@rationalsolutions.com \n To unsubscribe from all future mailing from Rational Business Solutions Inc., please send an email to\nunsubscribe@rationalsolutions.com\n \n\n\n\n\n \u0026nbsp; \n P Please consider our environment\n before printing.\u0026nbsp; \n \u0026nbsp; \n This e-mail may be privileged and/or confidential, and the sender does not waive any related rights and obligations. \n Any distribution, use or copying of this e-mail or the information it contains by other than an intended recipient is unauthorized. \n If you received this e-mail in error, please advise me (by return e-mail or otherwise) immediately \n \u0026nbsp; \n Please be advised that sometimes my SPAM filter eliminates legitimate email.\u0026nbsp; If your email contains important information, please ensure that you\n request acknowledgement of receipt. \n \n \u0026nbsp;","created_at":"2024-08-21T12:25:22Z"},{"author_id":17540439182993,"body":"case: 5680042\n\u0026nbsp;\nname: Kenneth\n\u0026nbsp;\nSUCCESS! PIN: YX4K2E - auth on phone\n\u0026nbsp;\nIssue:\n\u0026nbsp;\nhaven't received an errors from current backups, backup wasn't going since monday, the volume was unmounted but agent appears to be decrypted but with no volumes","created_at":"2024-08-21T12:40:28Z"},{"author_id":17540439182993,"body":"looks like it happened again :/ from partner explaining\n\u0026nbsp;\n conor.murphy@Weis-Backup03:~# zpool status -v\nno pools available\nconor.murphy@Weis-Backup03:~#","created_at":"2024-08-21T12:41:31Z"},{"author_id":17540439182993,"body":"Importing pool with -m flag since slog is missing\n\u0026nbsp;\n conor.murphy@Weis-Backup03:~# zpool import\n   pool: homePool\n     id: 4840822660934899508\n  state: UNAVAIL\nstatus: One or more devices are missing from the system.\n action: The pool cannot be imported. Attach the missing\n        devices and try again.\n   see: https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-6X\n config:\n\n        homePool                    UNAVAIL  missing device\n          mirror-0                  ONLINE\n            wwn-0x5000c500da104987  ONLINE\n            scsi-35000cca0c266a79c  ONLINE\n        logs\n          17598736664264038066      UNAVAIL\n\n        Additional devices are known to be part of this pool, though their\n        exact configuration cannot be determined.\nconor.murphy@Weis-Backup03:~# zpool import 4840822660934899508\nThe devices below are missing or corrupted, use '-m' to import the pool anyway:\n            ata-MZ7KH480HAHQ0D3_S5CNNA0T501177 [log]\n\ncannot import 'homePool': one or more devices is currently unavailable\nconor.murphy@Weis-Backup03:~# zpool import 4840822660934899508 -m\nconor.murphy@Weis-Backup03:~# zpool status -v\n  pool: homePool\n state: DEGRADED\nstatus: One or more devices could not be used because the label is missing or\n        invalid.  Sufficient replicas exist for the pool to continue\n        functioning in a degraded state.\naction: Replace the device using 'zpool replace'.\n   see: https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-4J\n  scan: resilvered 0B in 00:01:49 with 0 errors on Mon Nov  6 12:24:27 2023\nconfig:\n\n        NAME                        STATE     READ WRITE CKSUM\n        homePool                    DEGRADED     0     0     0\n          mirror-0                  ONLINE       0     0     0\n            wwn-0x5000c500da104987  ONLINE       0     0     0\n            scsi-35000cca0c266a79c  ONLINE       0     0     0\n        logs\n          17598736664264038066      UNAVAIL      0     0     0  was /dev/disk/by-id/ata-MZ7KH480HAHQ0D3_S5CNNA0T501177-part1\n\nerrors: No known data errors\nconor.murphy@Weis-Backup03:~# \u0026nbsp;\nlisting the drives\n conor.murphy@Weis-Backup03:~# lsscsi -s -u -i\n[0:0:0:0]    disk    5000c500da104987                  /dev/sda   35000c500da104987  8.00TB\n[0:0:1:0]    disk    5002538e0135761c                  /dev/sdb   -   480GB\n[0:0:2:0]    disk    5000cca0c266a79c                  /dev/sdc   35000cca0c266a79c  8.00TB\n[15:0:0:0]   disk    55cd2e415500f624                  /dev/sdd   -   240GB\n[16:0:0:0]   disk    55cd2e41562bd19f                  /dev/sde   -   240GB\n[17:0:0:0]   process none                              -          -       -\nconor.murphy@Weis-Backup03:~# \u0026nbsp;\ncan see the agent mountpoints\n homePool/home/agents/f898177883474688872d40412eb01a1f             6.5T  5.5T 1011G  85% /home/agents/f898177883474688872d40412eb01a1f\nhomePool/home/agents/b71c75516d1b4c06833aeb140bbf8317             1.3T  279G 1011G  22% /home/agents/b71c75516d1b4c06833aeb140bbf8317\nhomePool/home/agents/f898177883474688872d40412eb01a1f@1724020409  6.5T  5.5T 1011G  85% /home/agents/f898177883474688872d40412eb01a1f/.zfs/snapshot/1724020409\nhomePool/home/agents/b71c75516d1b4c06833aeb140bbf8317@1724020412  1.3T  279G 1011G  22% /home/agents/b71c75516d1b4c06833aeb140bbf8317/.zfs/snapshot/1724020412 \u0026nbsp;\nThere is a backup running\n conor.murphy@Weis-Backup03:~# snapctl system:zfs:repair\nEnsure that no backups are running and that backups are paused. Otherwise, data may be lost! Are you sure you wish to repair ZFS? [y/N] n\nconor.murphy@Weis-Backup03:~# \u0026nbsp;\nThey do have the canmount to on\n\u0026nbsp;\n conor.murphy@Weis-Backup03:~# zfs get all | grep -i canmount\nhomePool                                                          canmount              on                                                                default\nhomePool/.recv                                                    canmount              on                                                                default\nhomePool/home                                                     canmount              on                                                                default\nhomePool/home/agents                                              canmount              on                                                                default\nhomePool/home/agents/b71c75516d1b4c06833aeb140bbf8317             canmount              on                                                                local\nhomePool/home/agents/f898177883474688872d40412eb01a1f             canmount              on                                                                local\nhomePool/home/configBackup                                        canmount              on                                                                default\nhomePool/os                                                       canmount              on                                                                default\nhomePool/transfer                                                 canmount              on                                                                default\nconor.murphy@Weis-Backup03:~# \u0026nbsp;\nWe can get backup running\n\n\u0026nbsp;\nPartner isn't on site so explained we will need to do the chassis swap the 2nd time to see if we hit the slog PT or not","created_at":"2024-08-21T12:52:50Z"},{"author_id":17540439182993,"body":"Hello Ken,\n\u0026nbsp;\nThank you for calling Datto BCDR Support today.\n\u0026nbsp;\nYou called regarding the remount the pool due to the volumes showing as unmounted and you are going to go on-site to proceed with the second chassis swap replacement and we can check if this chassis has been imaged correctly, once on-site, give our support line a call and we can guide you through the process.\n\u0026nbsp;\nKind regards,\n\n\n  Conor Murphy | Technical Support Expert Level 1\n\nDatto, a Kaseya Company\n\nUSA: (833) 832 4780\n\nEMEA:\u0026nbsp;+44 (0) 118 402\u0026nbsp;9609\n\nAustralia: +61 (0)2 801 56826\n\nSingapore: 800-852-3047\n\nhelpdesk.kaseya.com","created_at":"2024-08-21T13:20:37Z"},{"author_id":17540439182993,"body":"Summary of case:\n\u0026nbsp;\nwe sent chassis 1 -\u0026gt; slog was image with OS -\u0026gt; we let them to continue working on chassis 1 -\u0026gt; we submitted an RMA for chassis 2 -\u0026gt; need partner to do chassis 1 to chassis 2 to see if it hits the PT / image correctly\n\u0026nbsp;\nThis ticket has now been opted in to the Support PCC process. PCC Automations will run on this ticket. Please submit as Pending.","created_at":"2024-08-21T13:20:58Z"},{"author_id":26547184729233,"body":"Auth'ed Ken while on the phone YNQ5H7","created_at":"2024-08-21T16:39:34Z"},{"author_id":26547184729233,"body":"call got disconnected somehow","created_at":"2024-08-21T16:52:41Z"},{"author_id":26547184729233,"body":"called the partner back.\n\u0026nbsp;\nWalked him through the steps.\u0026nbsp;\n\u0026nbsp;\nGet new device on and registered.\u0026nbsp;\nonce registered power it down and move all drives to the new chassis, except the OS Drive.\u0026nbsp;\n\u0026nbsp;\nPower the device back on.\u0026nbsp;","created_at":"2024-08-21T16:56:45Z"},{"author_id":26547184729233,"body":"cant connect to Datto front end or backend\n\n\u0026nbsp;\n\u0026nbsp;\n serge.augustin\u0026gt; c 84160C2930AA\nLooking up client ... Found: device-84160c2930aa Weis-Backup03 393451\nChecking for client ... Active and checking in.\nChecking for existing connection ... msa0ddh1l0n9eq9o (via dlt-rly-relay-6.datto.com)\nWaiting for connection ................................^[[A^[[B.^C Giving up. Cannot connect.\nserge.augustin\u0026gt; c 84160C2930AA\nLooking up client ... Found: device-84160c2930aa Weis-Backup03 393451\nChecking for client ... Not active. Connection will likely fail.\nChecking for existing connection ... msa0ddh1l0n9eq9o (via dlt-rly-relay-6.datto.com)\nWaiting for connection ....\nserge.augustin\u0026gt; c 84160C2930AA\nLooking up client ... Found: device-84160c2930aa Weis-Backup03 393451\nChecking for client ... Not active. Connection will likely fail.\nChecking for existing connection ... msa0ddh1l0n9eq9o (via dlt-rly-relay-6.datto.com)\nWaiting for connection ................................. Giving up. Cannot connect. \u0026nbsp;","created_at":"2024-08-21T17:24:21Z"},{"author_id":26547184729233,"body":"Hi Ken,\n\u0026nbsp;\n Here is the code 6B0SEL for our screen share \n\n  Serge Augustin | Technical Support | Monday - Friday 9:00am to 5:00pm EST. \n Datto, a Kaseya Company \n NA: +1 (888) 294-6312 \n EMEA: +44 (0) 118-402-9609 \n Australia: +61 (02) 8015-6826 \n DR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-21T17:27:33Z"},{"author_id":26547184729233,"body":"Hi Ken,\n\u0026nbsp;\nHere is the new code for the screen share\n\u0026nbsp;\n GDPZ4Z \n\n  Serge Augustin | Technical Support | Monday - Friday 9:00am to 5:00pm EST. \n Datto, a Kaseya Company \n NA: +1 (888) 294-6312 \n EMEA: +44 (0) 118-402-9609 \n Australia: +61 (02) 8015-6826 \n DR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-21T17:29:03Z"},{"author_id":26547184729233,"body":"partner getting networking error message when connecting with putty agent","created_at":"2024-08-21T17:40:14Z"},{"author_id":26547184729233,"body":"partner is rebooting the the datto device","created_at":"2024-08-21T17:45:04Z"},{"author_id":26547184729233,"body":"partner states he ran a network test earlier\u0026nbsp;\neverything passed expect check device host name resolution","created_at":"2024-08-21T17:46:17Z"},{"author_id":26547184729233,"body":"","created_at":"2024-08-21T17:48:27Z"},{"author_id":26547184729233,"body":"\u0026nbsp;\nthis is the IP that is showing on the Datto splash screen\u0026nbsp;\n192.168.2.156\n\u0026nbsp;","created_at":"2024-08-21T17:52:52Z"},{"author_id":12332349437457,"body":"new device 84160C2871FA\n\u0026nbsp;\nit's accessible from the backend\ndoing automated chassis swap Automated Chassis Swap\n\u0026nbsp;\ncompleted successfully\n\u0026nbsp;\nthe chassis went out without properly imaging the BOSS card...\n lmoulton@Weis-Backup04:~# lsscsi -s\n[0:0:0:0]    disk    SEAGATE  ST8000NM014A     CSLC  /dev/sda   8.00TB\n[0:0:1:0]    disk    ATA      SSDSC2KG960GZR   DL74  /dev/sdb    960GB\n[0:0:2:0]    disk    HGST     HUS728T8TAL5200  RS06  /dev/sdc   8.00TB\n[15:0:0:0]   disk    ATA      MTFDDAV240TDU    J004  /dev/sdd    240GB\n[16:0:0:0]   disk    ATA      MTFDDAV240TDU    J004  /dev/sde    240GB\n[17:0:0:0]   process Marvell  Console          1.01  -               -\nlmoulton@Weis-Backup04:~# blkid | grep -i stoneship\n/dev/sdd4: LABEL=\"Stoneship\" UUID=\"e7c7f8a5-8ac5-4104-89e5-307122a659c1\" TYPE=\"ext4\" PARTLABEL=\"Stoneship\" PARTUUID=\"16a16156-ec74-463e-9e4d-364d46f75549\"\nlmoulton@Weis-Backup04:~# \u0026nbsp;\nat least it's not on the SLOG drive; getting that set up at the very least\n\u0026nbsp;\n lmoulton@Weis-Backup04:~# ll /dev/disk/by-id/ | grep sdb\nlrwxrwxrwx 1 root root   9 Aug 21 13:44 ata-SSDSC2KG960GZR_PHYJ421602B2960BGN -\u0026gt; ../../sdb\nlrwxrwxrwx 1 root root   9 Aug 21 13:44 wwn-0x5c8d6b70003f4439 -\u0026gt; ../../sdb\nlmoulton@Weis-Backup04:~# zpool replace homePool 17598736664264038066 /dev/disk/by-id/ata-SSDSC2KG960GZR_PHYJ421602B2960BGN\nlmoulton@Weis-Backup04:~#\nlmoulton@Weis-Backup04:~# zpool status -v\n  pool: homePool\n state: ONLINE\nstatus: Some supported and requested features are not enabled on the pool.\n\tThe pool can still be used, but some features are unavailable.\naction: Enable all features using 'zpool upgrade'. Once this is done,\n\tthe pool may no longer be accessible by software that does not support\n\tthe features. See zpool-features(7) for details.\n  scan: resilvered 0B in 00:02:01 with 0 errors on Wed Aug 21 14:12:45 2024\nconfig:\n\n\tNAME                                     STATE     READ WRITE CKSUM\n\thomePool                                 ONLINE       0     0     0\n\t  mirror-0                               ONLINE       0     0     0\n\t    wwn-0x5000c500da104987               ONLINE       0     0     0\n\t    scsi-35000cca0c266a79c               ONLINE       0     0     0\n\tlogs\n\t  ata-SSDSC2KG960GZR_PHYJ421602B2960BGN  ONLINE       0     0     0\n\nerrors: No known data errors \u0026nbsp;","created_at":"2024-08-21T18:15:08Z"},{"author_id":12332349437457,"body":"case #5682395 ran into this same version of PT #5708398; an OS swap was successful there, so going to issue one here as well","created_at":"2024-08-21T18:23:35Z"},{"author_id":12332349437457,"body":"Hi Ken,\n\u0026nbsp;\nPer our call, the new chassis is set up and backups are running successfully! I have our cloud team working on reassociating the cloud data to this new chassis; we'll let you know when this is complete.\n\u0026nbsp;\nUnfortunately there seems to be an issue with the OS RAID on this new chassis; I've submitted a BOSS card swap for this so we can get this functioning in its best state.\n\u0026nbsp;\nWhen the replacement arrives, please call us from onsite so we can assist with the replacement. This article gives the procedure for swapping this out: Replacing the BOSS card or OS drive on SIRIS 5 - (6 to 36)\n\u0026nbsp;\n\n\n  Lillian Moulton | Technical Support L2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 863-2237\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-21T18:37:10Z"},{"author_id":12332349437457,"body":"reassoc complete already - finalizing\n lmoulton@Weis-Backup04:~# speedsync status\n+-------------------------------------------------------+------------+-----------+--------------+------------+----------+--------+\n| dataset                                               | target     | local zfs | remote files | remote zfs | priority | paused |\n+-------------------------------------------------------+------------+-----------+--------------+------------+----------+--------+\n| homePool/home/agents/b71c75516d1b4c06833aeb140bbf8317 | server1488 | 0         | 0            | 55         | medium   |        |\n| homePool/home/configBackup                            | server1488 | 0         | 0            | 551        | medium   |        |\n| homePool/home/agents/f898177883474688872d40412eb01a1f | server1488 | 0         | 0            | 55         | medium   |        |\n+-------------------------------------------------------+------------+-----------+--------------+------------+----------+--------+","created_at":"2024-08-21T18:51:04Z"},{"author_id":12332349437457,"body":"Hi Ken,\n\u0026nbsp;\nGood news - the offsite reassociation is already complete! I've enabled offsite sync.\n\u0026nbsp;\n\n\n  Lillian Moulton | Technical Support L2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 863-2237\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-21T18:53:17Z"},{"author_id":12332349437457,"body":"when they get the BOSS card, this is a full BOSS card swap - SIRIS 5: RAIDed OS drive swap says we don't have a procedure for it but the normal Automated OS Swap Procedure should work (see #5682395 for example)","created_at":"2024-08-21T18:54:39Z"},{"author_id":11408303966353,"body":"Hello Ken, \u0026nbsp;\n\u0026nbsp;\nThe tracking information was provided. \u0026nbsp;UPS - 1Z5RR6206673114917. \u0026nbsp;\n\u0026nbsp;\nBecause it was created recently, it could have errors on the tracking page. \u0026nbsp;Give it a little time and if still have an error, let us know.\u0026nbsp;\n\u0026nbsp;\nPlease give us a call once you get the component to continue with the replacement process. \u0026nbsp;\n\u0026nbsp;\nHave a great day!\u0026nbsp;\n\n\n  Jeremy Marrero | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 832-4780\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-21T21:03:04Z"},{"author_id":375501140238,"body":"Thanks Jeremy. Client is going to call UPS today to pick up the two chassis: \n original unit  \n sn : 84160C2930AA  \n UPS tracking : 1Z 5RR 620 91 7445 5296\n \n first chassis  \n sn : E43D1AA7924C  \n UPS tracking : 1Z 5RR 620 91 7576 2505 \n \u0026nbsp; \n \n Thanks. \n \u0026nbsp; \n Kenneth Lam \n \u0026nbsp; Senior Network Specialist \n  \n\n\n\n\n 275 Renfrew Drive \n Suite 105 \n Markham, ON\u0026nbsp;\n \n L3R 0C8 \n\n\n  \n\n\n \u0026nbsp; \n\n\n\n\n P: 905-771-8078 x202 \n F: 905-771-6756 \n E: \nklam@rationalsolutions.com \n To unsubscribe from all future mailing from Rational Business Solutions Inc., please send an email to\nunsubscribe@rationalsolutions.com\n \n\n\n\n\n \u0026nbsp; \n P Please consider our environment\n before printing.\u0026nbsp; \n \u0026nbsp; \n This e-mail may be privileged and/or confidential, and the sender does not waive any related rights and obligations. \n Any distribution, use or copying of this e-mail or the information it contains by other than an intended recipient is unauthorized. \n If you received this e-mail in error, please advise me (by return e-mail or otherwise) immediately \n \u0026nbsp; \n Please be advised that sometimes my SPAM filter eliminates legitimate email.\u0026nbsp; If your email contains important information, please ensure that you\n request acknowledgement of receipt. \n \n \u0026nbsp;","created_at":"2024-08-22T11:51:54Z"},{"author_id":11408303966353,"body":"Hello Ken, \u0026nbsp;\n\u0026nbsp;\nThanks for letting us know. \u0026nbsp;\n\u0026nbsp;\nWhen are you planing to perform the chassis swap?\u0026nbsp;\n\u0026nbsp;\nHave a great day!\u0026nbsp;\n\n\n  Jeremy Marrero | BCDR Technical Support Engineer Level 2\n\nDatto, a Kaseya Company\n\nNA: +1 (833) 832-4780\n\nEMEA: +44 (0) 118-402-9609\n\nAustralia: +61 (02) 8015-6826\n\nDR support: +1 (833) 328-8637\n\nhelpdesk.kaseya.com","created_at":"2024-08-22T22:35:19Z"}]}